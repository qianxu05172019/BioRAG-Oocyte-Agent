# BioRAG ç³»ç»ŸæŠ€æœ¯æ·±åº¦æ‹†è§£ â€” é€æ–‡ä»¶ã€é€å‡½æ•°ã€é€è¡Œè§£è¯»

> è¿™ä»½æ–‡æ¡£çš„ç›®çš„ï¼šè®©ä½ åœ¨é¢è¯•å‰å½»åº•ææ‡‚è‡ªå·±å†™çš„æ¯ä¸€è¡Œä»£ç ã€‚æ¯ä¸ªæ–‡ä»¶ã€æ¯ä¸ªç±»ã€æ¯ä¸ªå‡½æ•°éƒ½ä¼šè®²æ¸…æ¥š **åšäº†ä»€ä¹ˆ â†’ ä¸ºä»€ä¹ˆè¿™ä¹ˆåš â†’ è¾“å…¥è¾“å‡ºæ˜¯ä»€ä¹ˆ â†’ é¢è¯•å®˜ä¼šæ€ä¹ˆé—® â†’ ä½ åº”è¯¥æ€ä¹ˆå›ç­”**ã€‚

---

## ç›®å½•

1. [ç³»ç»Ÿå…¨æ™¯å›¾ â€” æ•°æ®æ€ä¹ˆä» PDF å˜æˆå›ç­”](#1-ç³»ç»Ÿå…¨æ™¯å›¾)
2. [æ–‡ä»¶ 1: `src/document_loader.py` â€” æ–‡æ¡£åŠ è½½ä¸åˆ†å—](#2-document_loaderpy)
3. [æ–‡ä»¶ 2: `src/embeddings.py` â€” å‘é‡å­˜å‚¨ç®¡ç†](#3-embeddingspy)
4. [æ–‡ä»¶ 3: `src/rag_pipeline.py` â€” RAG æ ¸å¿ƒæµæ°´çº¿](#4-rag_pipelinepy)
5. [æ–‡ä»¶ 4: `process_pdfs.py` â€” ç¦»çº¿é¢„å¤„ç†è„šæœ¬](#5-process_pdfspy)
6. [æ–‡ä»¶ 5: `app.py` â€” Streamlit Web åº”ç”¨](#6-apppy)
7. [æ–‡ä»¶ 6: `requirements.txt` â€” ä¾èµ–æ¸…å•](#7-requirementstxt)
8. [ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“](#8-ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“)

---

## 1. ç³»ç»Ÿå…¨æ™¯å›¾

å…ˆçœ‹å¤§å›¾ï¼Œä½ çš„ç³»ç»Ÿåˆ†ä¸¤ä¸ªé˜¶æ®µè¿è¡Œï¼š

```
é˜¶æ®µä¸€ï¼šç¦»çº¿é¢„å¤„ç†ï¼ˆåªè·‘ä¸€æ¬¡ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF è®ºæ–‡    â”‚ â†’  â”‚ DocumentProcessor â”‚ â†’  â”‚ VectorStoreManagerâ”‚ â†’  â”‚  ChromaDB    â”‚
â”‚  (data/pdfs) â”‚    â”‚ (åŠ è½½+åˆ†å—)       â”‚    â”‚ (ç¼–ç +å­˜å‚¨)        â”‚    â”‚ (æŒä¹…åŒ–å‘é‡åº“) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     process_pdfs.py é©±åŠ¨è¿™ä¸ªæµç¨‹

é˜¶æ®µäºŒï¼šåœ¨çº¿é—®ç­”ï¼ˆç”¨æˆ·æ¯æ¬¡æé—®ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·é—®é¢˜    â”‚ â†’  â”‚  ChromaDB    â”‚ â†’  â”‚ ConversationalRetrieval  â”‚ â†’  â”‚  å›ç­”+å¼•ç”¨  â”‚
â”‚  (è‡ªç„¶è¯­è¨€)  â”‚    â”‚ (æ£€ç´¢top-4)  â”‚    â”‚ Chain (GPT-3.5-turbo)    â”‚    â”‚  (å±•ç¤ºåœ¨UI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     app.py + rag_pipeline.py é©±åŠ¨è¿™ä¸ªæµç¨‹
```

é¢è¯•æ—¶è®°ä½è¿™ä¸ªä¸¤é˜¶æ®µæ¶æ„ã€‚é¢è¯•å®˜é—®"walk me through your system"ï¼Œå°±æŒ‰è¿™ä¸ªé¡ºåºè®²ã€‚

---

## 2. `document_loader.py`

è¿™æ˜¯ç³»ç»Ÿçš„**ç¬¬ä¸€ç«™**â€”â€”æŠŠ PDF æ–‡ä»¶å˜æˆ LLM èƒ½å¤„ç†çš„æ–‡æœ¬å—ã€‚

### å®Œæ•´æºç ï¼ˆ21 è¡Œï¼‰

```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )

    def load_pdfs(self, directory_path):
        """Load all PDFs from specified directory"""
        loader = DirectoryLoader(
            directory_path,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        documents = loader.load()
        return self.text_splitter.split_documents(documents)
```

### é€å—æ‹†è§£

#### Import éƒ¨åˆ†

```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `PyPDFLoader` | è¯»å–å•ä¸ª PDF æ–‡ä»¶ï¼Œæå–æ¯ä¸€é¡µçš„çº¯æ–‡æœ¬ |
| `DirectoryLoader` | æ‰«æä¸€ä¸ªç›®å½•ï¼Œæ‰¹é‡åŠ è½½æ‰€æœ‰åŒ¹é…æ–‡ä»¶ |
| `RecursiveCharacterTextSplitter` | æŠŠé•¿æ–‡æœ¬åˆ‡æˆå°å—ï¼Œæ˜¯ LangChain æœ€å¸¸ç”¨çš„åˆ†å—å™¨ |

#### `__init__` æ–¹æ³•

```python
def __init__(self):
    self.text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
```

**åšäº†ä»€ä¹ˆï¼š** åˆ›å»ºä¸€ä¸ªæ–‡æœ¬åˆ†å‰²å™¨å®ä¾‹ï¼Œé…ç½®å¥½åˆ†å—å‚æ•°ã€‚

**ä¸‰ä¸ªå‚æ•°çš„å«ä¹‰ï¼š**

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|----|------|
| `chunk_size` | 1000 | æ¯ä¸ªå—æœ€å¤š 1000 ä¸ªå­—ç¬¦ï¼ˆå¤§çº¦ 150-200 ä¸ªè‹±æ–‡å•è¯ï¼Œç›¸å½“äºä¸€ä¸ªè‡ªç„¶æ®µï¼‰ |
| `chunk_overlap` | 200 | ç›¸é‚»ä¸¤ä¸ªå—ä¹‹é—´æœ‰ 200 å­—ç¬¦çš„é‡å  |
| `length_function` | `len` | ç”¨ Python å†…ç½®çš„ `len()` æ¥è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼ˆæŒ‰å­—ç¬¦æ•°ï¼‰ |

**ä¸ºä»€ä¹ˆè¦é‡å ï¼ˆoverlapï¼‰ï¼Ÿ** ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾åŸæ–‡æ˜¯ï¼š
> "BMP15 is a key growth factor. It activates the SMAD signaling pathway, which regulates oocyte maturation."

å¦‚æœåœ¨ "pathway" ä¹‹ååˆšå¥½åˆ‡æ–­ï¼Œæ²¡æœ‰é‡å çš„è¯ï¼š
- å— A: "...It activates the SMAD signaling pathway,"
- å— B: "which regulates oocyte maturation."

å— B å­¤ç«‹åœ°çœ‹ï¼Œä½ ä¸çŸ¥é“"which"æŒ‡çš„æ˜¯ä»€ä¹ˆã€‚æœ‰äº† 200 å­—ç¬¦çš„é‡å ï¼š
- å— A: "...It activates the SMAD signaling pathway, which regulates oocyte maturation."
- å— B: "...SMAD signaling pathway, which regulates oocyte maturation. [åç»­å†…å®¹]..."

è¿™æ · BMP15 â†’ SMAD â†’ oocyte maturation çš„å…³ç³»åœ¨è‡³å°‘ä¸€ä¸ªå—ä¸­æ˜¯å®Œæ•´çš„ã€‚

**`RecursiveCharacterTextSplitter` çš„"é€’å½’"æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ** å®ƒä¼šæŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§åˆ†éš”ç¬¦æ¥åˆ‡å‰²ï¼š
1. å…ˆå°è¯• `\n\n`ï¼ˆæ®µè½åˆ†éš”ï¼‰
2. å†å°è¯• `\n`ï¼ˆæ¢è¡Œï¼‰
3. å†å°è¯• ` `ï¼ˆç©ºæ ¼ï¼‰
4. æœ€åé€å­—ç¬¦åˆ‡å‰²

è¿™æ ·å°½é‡åœ¨è¯­ä¹‰è‡ªç„¶çš„åœ°æ–¹æ–­å¼€ï¼Œè€Œä¸æ˜¯åœ¨å•è¯ä¸­é—´ç¡¬åˆ‡ã€‚

#### `load_pdfs` æ–¹æ³•

```python
def load_pdfs(self, directory_path):
    loader = DirectoryLoader(
        directory_path,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader
    )
    documents = loader.load()
    return self.text_splitter.split_documents(documents)
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | ä¾‹å­ |
|------|------|------|
| `directory_path` | `str` | `"data/pdfs"` |

**å†…éƒ¨æµç¨‹ï¼š**

```
ç¬¬1æ­¥: DirectoryLoader æ‰«æ data/pdfs/ ä¸‹æ‰€æœ‰ .pdf æ–‡ä»¶
       â†“
ç¬¬2æ­¥: å¯¹æ¯ä¸ª PDF ç”¨ PyPDFLoader é€é¡µæå–æ–‡æœ¬
       â†“ å¾—åˆ° List[Document]ï¼Œæ¯ä¸ª Document = ä¸€é¡µ PDF
       â†“ Document å¯¹è±¡æœ‰ä¸¤ä¸ªå±æ€§ï¼š
       â†“   .page_content = "è¿™ä¸€é¡µçš„æ–‡æœ¬å†…å®¹"
       â†“   .metadata = {"source": "data/pdfs/paper1.pdf", "page": 0}
       â†“
ç¬¬3æ­¥: text_splitter.split_documents() æŠŠæ¯é¡µæ–‡æœ¬åˆ‡æˆ ~1000 å­—ç¬¦çš„å—
       â†“ metadata ä¼šè¢«ç»§æ‰¿ï¼Œæ¯ä¸ªå—éƒ½çŸ¥é“è‡ªå·±æ¥è‡ªå“ªä¸ªæ–‡ä»¶ã€å“ªä¸€é¡µ
```

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| æ–‡æ¡£å—åˆ—è¡¨ | `List[Document]` | æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª ~1000 å­—ç¬¦çš„æ–‡æœ¬å—ï¼Œå¸¦æœ‰ source å’Œ page å…ƒæ•°æ® |

**å…·ä½“ä¾‹å­ï¼š** å‡è®¾ä½ æœ‰ 3 ç¯‡è®ºæ–‡ï¼Œæ¯ç¯‡ 10 é¡µï¼Œæ¯é¡µçº¦ 3000 å­—ç¬¦ï¼š
- `loader.load()` è¿”å› 30 ä¸ª Documentï¼ˆ3 ç¯‡ Ã— 10 é¡µï¼‰
- `split_documents()` æŠŠæ¯é¡µåˆ‡æˆçº¦ 3-4 ä¸ªå—ï¼ˆ3000 Ã· 1000ï¼Œè€ƒè™‘é‡å ï¼‰
- æœ€ç»ˆè¿”å›çº¦ 90-120 ä¸ªå—

### é¢è¯•é—®ç­”

---

**Q: Why did you choose RecursiveCharacterTextSplitter over other splitters?**

> "RecursiveCharacterTextSplitter is the most commonly recommended splitter in LangChain for general-purpose text, and here's why. It attempts to split at semantically meaningful boundaries â€” paragraph breaks first, then sentence breaks, then word breaks â€” rather than just cutting at a fixed character count. For scientific papers, this is particularly important because a key finding like 'BMP15 activates SMAD signaling' should ideally stay within a single chunk. The recursive approach maximizes the chance of that happening.
>
> Alternatives I considered: CharacterTextSplitter just splits on a single separator, which is less flexible. TokenTextSplitter splits by token count, which is useful when you need precise token budgets for the LLM, but character-based splitting was sufficient for my use case. If I were handling more structured documents, I might use something like MarkdownTextSplitter or a custom splitter that respects section headers like Abstract, Methods, Results."

---

**Q: Your chunk_size is 1000 characters. How did you arrive at that number?**

> "It's a trade-off between three factors. First, retrieval precision: smaller chunks mean each chunk is about one specific idea, so cosine similarity search is more precise. But too small â€” say 200 characters â€” and you lose context. A chunk that says 'it was upregulated' is useless without knowing what 'it' refers to. Second, LLM context window: each retrieved chunk consumes tokens. I retrieve 4 chunks, so that's roughly 4000 characters or about 1000 tokens â€” well within GPT-3.5-turbo's 16K token window, leaving plenty of room for conversation history and the generated answer. Third, semantic completeness: 1000 characters is roughly one paragraph in a scientific paper, which typically contains one complete idea or finding.
>
> I experimented with 500 and 1500. At 500, too many retrieval results were fragmentary. At 1500, irrelevant information was getting mixed in with relevant content. 1000 was the sweet spot for this corpus."

---

**Q: What happens if a PDF has images, tables, or equations?**

> "PyPDFLoader only extracts text content â€” it cannot parse images, table structures, or mathematical equations. This is a known limitation. For scientific papers, this means experimental data in tables and method details in figures are lost. To address this, I'd consider Unstructured.io for table extraction, a multimodal model like GPT-4V for figure description, or LlamaParse which is specifically designed for RAG-friendly document parsing. But for my MVP, text-only extraction was sufficient to demonstrate the core RAG workflow."

---

## 3. `embeddings.py`

è¿™æ˜¯ç³»ç»Ÿçš„**ç¬¬äºŒç«™**â€”â€”æŠŠæ–‡æœ¬å—å˜æˆå‘é‡ï¼Œå­˜è¿›æ•°æ®åº“ã€‚

### å®Œæ•´æºç ï¼ˆ35 è¡Œï¼‰

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
import os

class VectorStoreManager:
    def __init__(self):
        load_dotenv()
        os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
        self.embeddings = OpenAIEmbeddings()

    def create_vector_store(self, documents, persist_directory="data/chroma_db"):
        """Create or update vector store"""
        if not os.path.exists(persist_directory):
            os.makedirs(persist_directory)

        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=persist_directory
        )
        vector_store.persist()
        return vector_store

    def load_vector_store(self, persist_directory="data/chroma_db"):
        """Load existing vector store"""
        if not os.path.exists(persist_directory):
            raise ValueError("Vector store not found!")

        return Chroma(
            embedding_function=self.embeddings,
            persist_directory=persist_directory
        )
```

### é€å—æ‹†è§£

#### `__init__` æ–¹æ³•

```python
def __init__(self):
    load_dotenv()                                              # 1
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") # 2
    self.embeddings = OpenAIEmbeddings()                       # 3
```

ä¸‰è¡Œä»£ç åšäº†ä¸‰ä»¶äº‹ï¼š

| è¡Œ | åšäº†ä»€ä¹ˆ | ä¸ºä»€ä¹ˆ |
|----|---------|--------|
| 1 | ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ | `.env` é‡Œå­˜äº† `OPENAI_API_KEY=sk-xxxxx`ï¼Œè¿™è¡ŒæŠŠå®ƒè¯»è¿›å†…å­˜ |
| 2 | æŠŠ API Key è®¾è¿› `os.environ` | ç¡®ä¿ OpenAI SDK èƒ½é€šè¿‡ç¯å¢ƒå˜é‡æ‰¾åˆ° Key |
| 3 | åˆ›å»º OpenAI Embedding å®ä¾‹ | é»˜è®¤ä½¿ç”¨ `text-embedding-ada-002` æ¨¡å‹ï¼Œè¾“å‡º 1536 ç»´å‘é‡ |

**å…³äº Embedding æ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚ï¼š**
- æ¨¡å‹åï¼š`text-embedding-ada-002`
- è¾“å‡ºç»´åº¦ï¼š1536
- åŸç†ï¼šæŠŠä»»æ„é•¿åº¦çš„æ–‡æœ¬æ˜ å°„åˆ°ä¸€ä¸ª 1536 ç»´çš„æµ®ç‚¹æ•°å‘é‡
- æ€§è´¨ï¼šè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ â†’ å‘é‡åœ¨ç©ºé—´ä¸­è·ç¦»è¿‘ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦é«˜ï¼‰

ä¸¾ä¾‹ï¼š
```
"oocyte maturation process"  â†’  [0.012, -0.034, 0.056, ..., 0.078]  (1536ä¸ªæ•°)
"egg cell development"       â†’  [0.011, -0.031, 0.058, ..., 0.075]  (å¾ˆæ¥è¿‘!)
"weather forecast tomorrow"  â†’  [0.892, 0.445, -0.223, ..., -0.567] (å¾ˆè¿œ!)
```

#### `create_vector_store` æ–¹æ³•

```python
def create_vector_store(self, documents, persist_directory="data/chroma_db"):
    if not os.path.exists(persist_directory):
        os.makedirs(persist_directory)

    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=self.embeddings,
        persist_directory=persist_directory
    )
    vector_store.persist()
    return vector_store
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `documents` | `List[Document]` | ä¸Šä¸€æ­¥ `DocumentProcessor.load_pdfs()` è¿”å›çš„æ–‡æœ¬å—åˆ—è¡¨ |
| `persist_directory` | `str` | å‘é‡åº“å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ `"data/chroma_db"` |

**å†…éƒ¨æµç¨‹ï¼š**

```
ç¬¬1æ­¥: æ£€æŸ¥å­˜å‚¨ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±åˆ›å»º
       â†“
ç¬¬2æ­¥: Chroma.from_documents() åšäº†ä¸¤ä»¶äº‹ï¼š
       a) å¯¹æ¯ä¸ª Document çš„ page_content è°ƒç”¨ OpenAI Embedding API
          "BMP15 activates SMAD..." â†’ [0.012, -0.034, ..., 0.078] (1536ç»´)
       b) æŠŠå‘é‡ + åŸæ–‡ + metadata å­˜è¿› ChromaDB
       â†“
ç¬¬3æ­¥: vector_store.persist() æŠŠå†…å­˜ä¸­çš„æ•°æ®å†™åˆ°ç£ç›˜
       ç”Ÿæˆæ–‡ä»¶åœ¨ data/chroma_db/ ç›®å½•ä¸‹
```

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| `vector_store` | `Chroma` | å¯ä»¥ç›´æ¥ç”¨æ¥åšç›¸ä¼¼åº¦æœç´¢çš„å‘é‡åº“å¯¹è±¡ |

**å®é™…å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ** å‡è®¾ä½ æœ‰ 100 ä¸ªæ–‡æœ¬å—ï¼š
- å‘ OpenAI API å‘é€ 100 æ¬¡ Embedding è¯·æ±‚ï¼ˆæˆ–æ‰¹é‡å‘é€ï¼‰
- æ¯ä¸ªå—å˜æˆ 1536 ä¸ªæµ®ç‚¹æ•°
- 100 ä¸ªå‘é‡ + 100 æ®µåŸæ–‡ + 100 æ¡ metadata å­˜å…¥ ChromaDB
- æ•°æ®æŒä¹…åŒ–åˆ° `data/chroma_db/` ç›®å½•ï¼ˆSQLite + ç´¢å¼•æ–‡ä»¶ï¼‰

#### `load_vector_store` æ–¹æ³•

```python
def load_vector_store(self, persist_directory="data/chroma_db"):
    if not os.path.exists(persist_directory):
        raise ValueError("Vector store not found!")

    return Chroma(
        embedding_function=self.embeddings,
        persist_directory=persist_directory
    )
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `persist_directory` | `str` | ä¹‹å‰åˆ›å»ºçš„å‘é‡åº“è·¯å¾„ |

**åšäº†ä»€ä¹ˆï¼š** ä»ç£ç›˜åŠ è½½å·²æœ‰çš„å‘é‡åº“ã€‚æ³¨æ„å®ƒä¸éœ€è¦é‡æ–°ç¼–ç æ–‡æ¡£â€”â€”å‘é‡å·²ç»å­˜å¥½äº†ã€‚å®ƒåªéœ€è¦ `embedding_function` æ˜¯å› ä¸ºåç»­æŸ¥è¯¢æ—¶éœ€è¦æŠŠç”¨æˆ·çš„é—®é¢˜ä¹Ÿç¼–ç æˆå‘é‡ã€‚

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| `Chroma` å¯¹è±¡ | `Chroma` | ä»ç£ç›˜æ¢å¤çš„å‘é‡åº“ï¼Œå¯ä»¥ç›´æ¥æœç´¢ |

**`create` vs `load` çš„åŒºåˆ«ï¼š**
- `create_vector_store`ï¼šç¬¬ä¸€æ¬¡è·‘ï¼Œä»é›¶å¼€å§‹ç¼–ç æ–‡æ¡£å¹¶å­˜å‚¨ã€‚æ…¢ï¼ˆè¦è°ƒ APIï¼‰ã€‚
- `load_vector_store`ï¼šåç»­æ¯æ¬¡å¯åŠ¨åº”ç”¨æ—¶ç”¨ï¼Œåªæ˜¯ä»ç£ç›˜è¯»å–å·²æœ‰æ•°æ®ã€‚å¿«ï¼ˆä¸è°ƒ APIï¼‰ã€‚

### é¢è¯•é—®ç­”

---

**Q: Why ChromaDB and not Pinecone, FAISS, or Weaviate?**

> "I chose ChromaDB for three reasons specific to this project's constraints. First, local persistence: ChromaDB stores data as files on disk â€” no need for a running server process or cloud service. For a prototype with three papers, this is ideal. Second, LangChain integration: ChromaDB has first-class support in LangChain â€” `Chroma.from_documents()` handles embedding and indexing in a single call. Third, zero infrastructure: no Docker, no API keys for the database, no cost.
>
> The trade-offs are clear. ChromaDB doesn't support horizontal scaling â€” if I had millions of vectors, I'd need Pinecone for managed scaling or Milvus for self-hosted distributed search. FAISS, from Facebook, would give me better raw search performance, but it doesn't have built-in persistence â€” I'd need to manage save/load logic myself. For this project's scope â€” thousands of vectors, single-user â€” ChromaDB was the right call."

---

**Q: What embedding model are you using, and what are its characteristics?**

> "I'm using OpenAI's text-embedding-ada-002, which outputs 1536-dimensional dense vectors. It's trained on a large corpus with a contrastive learning objective, meaning it learns to place semantically similar texts close together in vector space and dissimilar texts far apart. It supports up to 8191 tokens of input.
>
> Key characteristics: it's multilingual, so a query in English can match a passage about the same concept in Chinese. It produces normalized vectors, which means cosine similarity and dot product give the same ranking. And it's relatively cheap â€” about $0.0001 per 1000 tokens.
>
> If I were to improve this, I'd consider domain-specific embedding models like BiomedBERT or PubMedBERT embeddings, which are fine-tuned on biomedical literature and might better capture specialized terminology like gene names and pathway terms."

---

**Q: What does `persist()` actually do under the hood?**

> "ChromaDB stores vectors in memory during the session. `persist()` flushes that in-memory data to disk in the `persist_directory`. Under the hood, ChromaDB uses SQLite for metadata storage and a custom index format for the vectors. After `persist()` is called, the directory contains files like `chroma.sqlite3` for metadata and index files for the vector index. This means if the process restarts, we can reload the exact same state without re-computing embeddings â€” which saves both time and API costs."

---

## 4. `rag_pipeline.py`

è¿™æ˜¯ç³»ç»Ÿçš„**å¤§è„‘**â€”â€”æŠŠæ£€ç´¢å’Œç”Ÿæˆä¸²åœ¨ä¸€èµ·ï¼Œå®ç°å¯¹è¯å¼é—®ç­”ã€‚

### å®Œæ•´æºç 

```python
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

class RAGPipeline:
    def __init__(self, vector_store):
        # æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„å‘é‡åº“å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        self.vector_store = vector_store

        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        self.qa_chain = ConversationalRetrievalChain.from_llm(
            ChatOpenAI(temperature=0),
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 4}),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )

    def ask(self, query: str):
        response = self.qa_chain({"question": query})
        return response

if __name__ == '__main__':
    from src.embeddings import VectorStoreManager
    vector_store_manager = VectorStoreManager()
    vector_store = vector_store_manager.load_vector_store()
    rag = RAGPipeline(vector_store)
    while True:
        user_input = input("Please type your question:")
        if user_input.lower() == 'exit':
            break
        result = rag.ask(user_input)
        print("Answerï¼š", result['answer'])
```

### é€å—æ‹†è§£

#### `__init__` æ–¹æ³• â€” ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„ç»„è£…

è¿™ä¸ª `__init__` ç»„è£…äº†æ•´ä¸ª RAG æµæ°´çº¿çš„ä¸‰ä¸ªå…³é”®é›¶ä»¶ï¼š

**é›¶ä»¶ 1: å‘é‡åº“ï¼ˆRetrieverï¼‰**
```python
self.vector_store = vector_store
```
æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„å‘é‡åº“å®ä¾‹ï¼ˆç”± `VectorStoreManager.load_vector_store()` åˆ›å»ºï¼‰ã€‚è¿™æ ·é¿å…äº†é‡å¤åˆ›å»º Chroma è¿æ¥å’Œ Embedding æ¨¡å‹ï¼Œå®ç°äº†èŒè´£åˆ†ç¦»ï¼š`VectorStoreManager` è´Ÿè´£ç®¡ç†å‘é‡åº“çš„åˆ›å»ºå’ŒåŠ è½½ï¼Œ`RAGPipeline` åªè´Ÿè´£é—®ç­”ã€‚

**é›¶ä»¶ 2: å¯¹è¯è®°å¿†**
```python
self.memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|----|------|
| `memory_key` | `"chat_history"` | è®°å¿†å­˜å‚¨åœ¨å­—å…¸çš„ `chat_history` é”®ä¸‹ |
| `return_messages` | `True` | ä»¥ Message å¯¹è±¡åˆ—è¡¨çš„æ ¼å¼è¿”å›å†å²ï¼ˆè€Œä¸æ˜¯çº¯å­—ç¬¦ä¸²æ‹¼æ¥ï¼‰ |

**ConversationBufferMemory çš„å·¥ä½œæ–¹å¼ï¼š**

```
ç¬¬1è½®:
  ç”¨æˆ·: "What is BMP15?"
  AI: "BMP15 is a growth factor..."
  â†’ memory å­˜å‚¨: [HumanMessage("What is BMP15?"), AIMessage("BMP15 is a growth factor...")]

ç¬¬2è½®:
  ç”¨æˆ·: "How does it relate to oocyte maturation?"
  â†’ memory æŠŠä¹‹å‰çš„è®°å½•ä¸€èµ·å‘ç»™ LLMï¼Œè¿™æ · LLM çŸ¥é“ "it" = BMP15

ç¬¬3è½®:
  â†’ memory è¶Šæ¥è¶Šé•¿... è¿™å°±æ˜¯ BufferMemory çš„é—®é¢˜
```

**é›¶ä»¶ 3: å¯¹è¯æ£€ç´¢é“¾ï¼ˆæ ¸å¿ƒï¼ï¼‰**
```python
self.qa_chain = ConversationalRetrievalChain.from_llm(
    ChatOpenAI(temperature=0),                                      # LLM
    retriever=self.vector_store.as_retriever(search_kwargs={"k": 4}), # æ£€ç´¢å™¨
    memory=self.memory,                                              # è®°å¿†
    return_source_documents=True,                                     # è¿”å›å¼•ç”¨
    verbose=True                                                      # æ‰“å°è°ƒè¯•æ—¥å¿—
)
```

**æ¯ä¸ªå‚æ•°çš„ä½œç”¨ï¼š**

| å‚æ•° | å€¼ | ä½œç”¨ |
|------|----|------|
| `ChatOpenAI(temperature=0)` | GPT-3.5-turbo | è´Ÿè´£ç”Ÿæˆå›ç­”ã€‚temperature=0 â†’ ç¡®å®šæ€§è¾“å‡ºï¼Œä¸è¦åˆ›é€ æ€§ |
| `retriever` | ChromaDB retriever | è´Ÿè´£ä»å‘é‡åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚`k=4` è¡¨ç¤ºè¿”å›æœ€ç›¸ä¼¼çš„ 4 ä¸ªå— |
| `memory` | ConversationBufferMemory | ç»´æŠ¤å¯¹è¯å†å²ï¼Œæ”¯æŒå¤šè½®é—®ç­” |
| `return_source_documents` | `True` | åœ¨å“åº”ä¸­é™„å¸¦æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£ï¼ˆç”¨äºå±•ç¤ºå¼•ç”¨ï¼‰ |
| `verbose` | `True` | åœ¨ç»ˆç«¯æ‰“å° Chain çš„æ‰§è¡Œæ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰ |

**`as_retriever(search_kwargs={"k": 4})` åšäº†ä»€ä¹ˆï¼Ÿ**
æŠŠ ChromaDB å‘é‡åº“åŒ…è£…æˆä¸€ä¸ª LangChain `Retriever` å¯¹è±¡ã€‚å½“è¢«è°ƒç”¨æ—¶ï¼š
1. æ¥æ”¶æŸ¥è¯¢æ–‡æœ¬
2. ç”¨ `self.embeddings` æŠŠæŸ¥è¯¢ç¼–ç æˆå‘é‡
3. åœ¨ ChromaDB ä¸­åšä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
4. è¿”å› top-4 æœ€ç›¸ä¼¼çš„ Document å¯¹è±¡

**ConversationalRetrievalChain å†…éƒ¨çš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆé‡è¦ï¼é¢è¯•å¿…è€ƒï¼‰ï¼š**

```
ç”¨æˆ·è¾“å…¥: "Are any of those druggable?"
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Question Condensingï¼ˆé—®é¢˜é‡å†™ï¼‰       â”‚
â”‚                                              â”‚
â”‚  è¾“å…¥: å½“å‰é—®é¢˜ + chat_history                 â”‚
â”‚    "Are any of those druggable?"             â”‚
â”‚    + [ä¹‹å‰é—®äº† "What pathways regulate        â”‚
â”‚       oocyte maturation?"]                    â”‚
â”‚                                              â”‚
â”‚  â†’ LLM é‡å†™ä¸ºç‹¬ç«‹é—®é¢˜:                         â”‚
â”‚    "Are any pathways that regulate oocyte     â”‚
â”‚     maturation associated with druggable      â”‚
â”‚     targets?"                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Retrievalï¼ˆå‘é‡æ£€ç´¢ï¼‰                 â”‚
â”‚                                              â”‚
â”‚  ç”¨é‡å†™åçš„ç‹¬ç«‹é—®é¢˜å» ChromaDB æœç´¢              â”‚
â”‚  â†’ è¿”å› top-4 æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—                    â”‚
â”‚  æ¯ä¸ªå—å¸¦æœ‰ page_content å’Œ metadata           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Generationï¼ˆç­”æ¡ˆç”Ÿæˆï¼‰                â”‚
â”‚                                              â”‚
â”‚  æŠŠ 4 ä¸ªæ–‡æœ¬å— + é‡å†™åçš„é—®é¢˜ ä¸€èµ·å‘ç»™ GPT-3.5  â”‚
â”‚  Prompt å¤§è‡´æ˜¯:                                â”‚
â”‚    "Based on the following context, answer    â”‚
â”‚     the question.                             â”‚
â”‚     Context: [4ä¸ªæ–‡æœ¬å—]                       â”‚
â”‚     Question: [é‡å†™åçš„é—®é¢˜]"                   â”‚
â”‚                                              â”‚
â”‚  â†’ GPT-3.5 åŸºäºè¿™äº›ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Memory Updateï¼ˆè®°å¿†æ›´æ–°ï¼‰             â”‚
â”‚                                              â”‚
â”‚  æŠŠè¿™è½®çš„é—®ç­”å¯¹è¿½åŠ åˆ° chat_history               â”‚
â”‚  ä¸‹æ¬¡æé—®æ—¶ä¼šå¸¦ä¸Šå®Œæ•´çš„å¯¹è¯å†å²                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆ Step 1ï¼ˆé—®é¢˜é‡å†™ï¼‰æ˜¯å…³é”®ï¼Ÿ**

æ²¡æœ‰é—®é¢˜é‡å†™çš„è¯ï¼š
- ç”¨æˆ·é—®ï¼š"Are any of those druggable?"
- "those" å¯¹å‘é‡åº“æ¥è¯´æ¯«æ— æ„ä¹‰
- æ£€ç´¢ç»“æœä¼šå®Œå…¨ä¸ç›¸å…³
- å›ç­”è´¨é‡å´©æºƒ

æœ‰é—®é¢˜é‡å†™åï¼š
- LLM æŠŠ "those" è§£æä¸ºä¹‹å‰æåˆ°çš„ pathways
- ç”Ÿæˆç‹¬ç«‹é—®é¢˜ï¼š"Are any pathways regulating oocyte maturation druggable?"
- å‘é‡åº“æ£€ç´¢åˆ°æ­£ç¡®çš„æ–‡æ¡£
- å›ç­”è´¨é‡ä¿æŒé«˜æ°´å¹³

#### `ask` æ–¹æ³•

```python
def ask(self, query: str):
    response = self.qa_chain({"question": query})
    return response
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | ä¾‹å­ |
|------|------|------|
| `query` | `str` | `"What signaling pathways regulate oocyte maturation?"` |

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | ç»“æ„ |
|--------|------|------|
| `response` | `dict` | åŒ…å«ä¸‰ä¸ªé”®ï¼ˆè§ä¸‹è¡¨ï¼‰ |

```python
{
    "question": "What signaling pathways regulate oocyte maturation?",
    "answer": "Based on the literature, several signaling pathways regulate oocyte maturation, including the MAPK/ERK pathway, PI3K/AKT pathway, and BMP/SMAD pathway...",
    "source_documents": [
        Document(page_content="...", metadata={"source": "paper1.pdf", "page": 3}),
        Document(page_content="...", metadata={"source": "paper2.pdf", "page": 7}),
        Document(page_content="...", metadata={"source": "paper1.pdf", "page": 5}),
        Document(page_content="...", metadata={"source": "paper3.pdf", "page": 2})
    ]
}
```

#### `__main__` éƒ¨åˆ†

```python
if __name__ == '__main__':
    from src.embeddings import VectorStoreManager
    vector_store_manager = VectorStoreManager()
    vector_store = vector_store_manager.load_vector_store()
    rag = RAGPipeline(vector_store)
    while True:
        user_input = input("Please type your question:")
        if user_input.lower() == 'exit':
            break
        result = rag.ask(user_input)
        print("Answerï¼š", result['answer'])
```

è¿™æ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œæµ‹è¯•æ¥å£ã€‚ç›´æ¥è¿è¡Œ `python src/rag_pipeline.py` å°±å¯ä»¥åœ¨ç»ˆç«¯é‡Œå’Œç³»ç»Ÿå¯¹è¯ï¼Œä¸éœ€è¦å¯åŠ¨ Streamlitã€‚æ–¹ä¾¿è°ƒè¯•ã€‚æ³¨æ„è¿™é‡Œé€šè¿‡ `VectorStoreManager` åŠ è½½å‘é‡åº“å†ä¼ ç»™ `RAGPipeline`ï¼Œä¿æŒäº†å’Œ `app.py` ä¸€è‡´çš„åˆå§‹åŒ–æ–¹å¼ã€‚

### é¢è¯•é—®ç­”

---

**Q: Walk me through what happens internally when a user asks a question.**

> "When the user submits a query, the ConversationalRetrievalChain executes a three-step pipeline. First, question condensing: if there's any conversation history, the chain sends the current question along with the chat history to the LLM and asks it to rewrite the question as a standalone query. For example, 'How does it affect fertility?' becomes 'How does BMP15 affect oocyte fertility?' This is critical because the vector database has no concept of conversational context.
>
> Second, retrieval: the condensed question is embedded using OpenAI's embedding model into a 1536-dimensional vector, then ChromaDB performs a cosine similarity search and returns the top-4 most similar document chunks. Each chunk carries metadata including the source file and page number.
>
> Third, generation: the 4 retrieved chunks are injected into a prompt template as context, along with the condensed question. GPT-3.5-turbo generates an answer grounded in that context. Finally, the memory module stores this Q&A pair for future turns."

---

**Q: What's the problem with ConversationBufferMemory? How would you fix it?**

> "ConversationBufferMemory stores the entire conversation history verbatim. Every human message and AI response is appended to a growing list. After 10-15 exchanges, this can consume several thousand tokens, eating into GPT-3.5-turbo's 16K context window and leaving less room for retrieved documents and the actual answer.
>
> I'd fix this in stages. The quickest fix is ConversationBufferWindowMemory with k=5 â€” keep only the last 5 exchanges, drop everything older. A more sophisticated approach is ConversationSummaryBufferMemory â€” it keeps recent messages verbatim but summarizes older ones using the LLM, compressing 'we discussed BMP15, SMAD signaling, and oocyte maturation pathways' into a single sentence. The best production approach is a hybrid: recent 3 turns kept in full, everything older summarized, with a hard token cap."

---

**Q: Why `temperature=0`? Would you ever change it?**

> "Temperature controls the probability distribution over the next token during generation. At zero, the model always picks the most probable token â€” effectively greedy decoding. This makes output deterministic and maximally factual, which is essential for scientific Q&A. A researcher asking about cell signaling pathways needs a reproducible, evidence-based answer, not creative prose.
>
> I'd increase temperature in specific scenarios: hypothesis generation, where you want the model to suggest non-obvious connections, maybe 0.3 to 0.5. Or diverse summarization, where you want multiple distinct phrasings of the same concept. But for any customer-facing QIAGEN product serving pharma researchers, I'd default to low temperature. Reproducibility is a requirement, not a nice-to-have."

---

**Q: What does `return_source_documents=True` give you?**

> "It instructs the chain to include the actual Document objects that were retrieved from ChromaDB in the response dictionary. Each Document has two attributes: `page_content`, which is the raw text of that chunk, and `metadata`, which contains the source PDF filename and page number. This is what powers the citation feature in the UI â€” I can show users not just the answer, but exactly which passages from which papers the answer is based on. In a pharma context, this traceability is non-negotiable. A scientist needs to verify any AI-generated claim against the primary source."

---

## 5. `process_pdfs.py`

è¿™æ˜¯ä¸€ä¸ª**ä¸€æ¬¡æ€§è¿è¡Œçš„è„šæœ¬**â€”â€”æ‰§è¡Œé˜¶æ®µä¸€ï¼ˆç¦»çº¿é¢„å¤„ç†ï¼‰ï¼ŒæŠŠ PDF å˜æˆå‘é‡åº“ã€‚

### å®Œæ•´æºç ï¼ˆ49 è¡Œï¼‰

```python
from src.document_loader import DocumentProcessor
from src.embeddings import VectorStoreManager
import os

def main():
    pdf_directory = "data/pdfs"

    if not os.path.exists(pdf_directory):
        os.makedirs(pdf_directory)
        print(f"Created directory {pdf_directory}")
        print(f"Please place your PDF files in {pdf_directory} directory and run this script again.")
        return

    pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print(f"No PDF files found in {pdf_directory}!")
        print("Please add some PDF files to this directory and run this script again.")
        return

    print(f"Found {len(pdf_files)} PDF files: {', '.join(pdf_files)}")

    print("Processing PDF documents...")
    document_processor = DocumentProcessor()
    documents = document_processor.load_pdfs(pdf_directory)

    if not documents:
        print("Error: No document chunks were generated!")
        return

    print(f"Successfully processed {len(documents)} document chunks.")

    print("Creating vector store (this may take a while)...")
    try:
        vector_store_manager = VectorStoreManager()
        vector_store = vector_store_manager.create_vector_store(documents)
        print("Vector store created successfully!")
        print("You can now run the Streamlit app and query your documents.")
    except Exception as e:
        print(f"Error creating vector store: {str(e)}")
        print("Check your OpenAI API key and ensure it's correctly set in your .env file.")

if __name__ == "__main__":
    main()
```

### é€å—æ‹†è§£

è¿™ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ª**ç¼–æ’è„šæœ¬**â€”â€”å®ƒæœ¬èº«ä¸åŒ…å«æ–°é€»è¾‘ï¼Œè€Œæ˜¯æŒ‰æ­£ç¡®çš„é¡ºåºè°ƒç”¨å‰é¢ä¸¤ä¸ªæ¨¡å—ã€‚

**å®Œæ•´æ‰§è¡Œæµç¨‹ï¼š**

```
python process_pdfs.py
        â†“
æ£€æŸ¥ data/pdfs/ ç›®å½•æ˜¯å¦å­˜åœ¨ â†’ ä¸å­˜åœ¨å°±åˆ›å»ºå¹¶æç¤ºç”¨æˆ·æ”¾å…¥ PDF
        â†“
æ£€æŸ¥ç›®å½•é‡Œæœ‰æ²¡æœ‰ .pdf æ–‡ä»¶ â†’ æ²¡æœ‰å°±æç¤ºç”¨æˆ·æ·»åŠ 
        â†“
DocumentProcessor().load_pdfs("data/pdfs")
  â†’ è¯»å–æ‰€æœ‰ PDF â†’ æå–æ–‡æœ¬ â†’ åˆ‡åˆ†æˆ ~1000 å­—ç¬¦çš„å—
  â†’ è¿”å› List[Document]
        â†“
æ‰“å° "Successfully processed X document chunks."
        â†“
VectorStoreManager().create_vector_store(documents)
  â†’ å¯¹æ¯ä¸ªå—è°ƒç”¨ OpenAI Embedding API â†’ å­˜å…¥ ChromaDB â†’ æŒä¹…åŒ–åˆ°ç£ç›˜
        â†“
æ‰“å° "Vector store created successfully!"
        â†“
ç°åœ¨å¯ä»¥è¿è¡Œ streamlit run app.py äº†
```

**ä¸ºä»€ä¹ˆè¦æŠŠé¢„å¤„ç†ç‹¬ç«‹æˆä¸€ä¸ªè„šæœ¬ï¼Ÿ**

ä¸‰ä¸ªåŸå› ï¼š
1. **Embedding å¾ˆè´µä¹Ÿå¾ˆæ…¢ï¼š** 100 ä¸ªæ–‡æœ¬å—éœ€è¦çº¦ 100 æ¬¡ API è°ƒç”¨ã€‚å¦‚æœæ¯æ¬¡å¯åŠ¨ app éƒ½é‡æ–°ç¼–ç ï¼Œæµªè´¹æ—¶é—´å’Œé’±ã€‚
2. **æ•°æ®ä¸å¸¸å˜ï¼š** è®ºæ–‡åº“ä¸æ˜¯æ¯å¤©éƒ½æ›´æ–°çš„ã€‚é¢„å¤„ç†ä¸€æ¬¡ï¼Œå‘é‡åº“å­˜åˆ°ç£ç›˜ï¼Œapp å¯åŠ¨æ—¶ç›´æ¥åŠ è½½ã€‚
3. **å…³æ³¨ç‚¹åˆ†ç¦»ï¼š** æ•°æ®å‡†å¤‡å’Œç”¨æˆ·äº¤äº’æ˜¯ä¸¤ä¸ªä¸åŒçš„å…³æ³¨ç‚¹ã€‚åˆ†å¼€åï¼Œä½ å¯ä»¥å•ç‹¬æ›´æ–°æ•°æ®ï¼ˆåŠ æ–°è®ºæ–‡ â†’ é‡è·‘ `process_pdfs.py`ï¼‰ï¼Œè€Œä¸å½±å“ app çš„ä»£ç ã€‚

**é˜²å¾¡æ€§ç¼–ç¨‹ç»†èŠ‚ï¼š**
- æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
- æ£€æŸ¥æ˜¯å¦æœ‰ PDF æ–‡ä»¶
- æ£€æŸ¥åˆ†å—ç»“æœæ˜¯å¦ä¸ºç©º
- æ•è·å‘é‡å­˜å‚¨åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¼‚å¸¸ï¼ˆé€šå¸¸æ˜¯ API Key é—®é¢˜ï¼‰

### é¢è¯•é—®ç­”

---

**Q: Why did you separate the PDF processing from the main app?**

> "It's a classic separation of offline and online workloads. Embedding documents is an expensive, one-time operation â€” it calls the OpenAI API for every chunk and can take minutes. The web application, on the other hand, needs to start quickly and serve user queries in real time. By processing PDFs offline and persisting the vector store to disk, the app startup just loads pre-computed vectors from disk â€” instant. This also saves API costs: if I restart the app 10 times during development, I'm not re-embedding the same documents 10 times. In a production system, this separation would be even more important â€” you'd likely run the ingestion pipeline on a schedule or as a triggered job, completely decoupled from the serving layer."

---

## 6. `app.py`

è¿™æ˜¯**ç”¨æˆ·çœ‹åˆ°çš„ç•Œé¢**â€”â€”Streamlit Web åº”ç”¨ï¼ŒæŠŠæ‰€æœ‰åç«¯æ¨¡å—ä¸²æˆä¸€ä¸ªå¯äº¤äº’çš„äº§å“ã€‚

### å®Œæ•´æºç ï¼ˆ152 è¡Œï¼‰â€” åˆ†å››ä¸ªé€»è¾‘æ®µè½è®²è§£

#### æ®µè½ 1: é…ç½®ä¸æ ·å¼ï¼ˆç¬¬ 1-42 è¡Œï¼‰

```python
import streamlit as st
from src.embeddings import VectorStoreManager
from src.rag_pipeline import RAGPipeline
import os

if not os.getenv("OPENAI_API_KEY"):
    raise EnvironmentError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–åœ¨ .env æ–‡ä»¶ä¸­æä¾›")

st.set_page_config(
    page_title="Oocyte Expert",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""<style>...</style>""", unsafe_allow_html=True)
```

**åšäº†ä»€ä¹ˆï¼š**
1. **ç¯å¢ƒæ£€æŸ¥ï¼š** å¯åŠ¨æ—¶ç«‹å³æ£€æŸ¥ API Key æ˜¯å¦å­˜åœ¨ï¼Œæ²¡æœ‰å°±ç›´æ¥æŠ¥é”™é€€å‡ºã€‚è¿™æ˜¯å®‰å…¨å®è·µâ€”â€”fail fastã€‚
2. **é¡µé¢é…ç½®ï¼š** `st.set_page_config()` è®¾ç½®æµè§ˆå™¨æ ‡ç­¾æ ‡é¢˜ã€å›¾æ ‡ã€é¡µé¢å¸ƒå±€ã€‚`layout="wide"` è®©é¡µé¢ä½¿ç”¨å…¨å®½è€Œä¸æ˜¯é»˜è®¤çš„å±…ä¸­çª„åˆ—ã€‚
3. **è‡ªå®šä¹‰ CSSï¼š** é€šè¿‡ `st.markdown` æ³¨å…¥ CSS æ¥ç¾åŒ–èŠå¤©ç•Œé¢ã€‚å®šä¹‰äº† `.chat-message`ã€`.user-message`ã€`.assistant-message` å’Œ `.citation` å››ä¸ªæ ·å¼ç±»ã€‚

#### æ®µè½ 2: Session State åˆå§‹åŒ–ï¼ˆç¬¬ 44-52 è¡Œï¼‰

```python
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = None
if 'is_initialized' not in st.session_state:
    st.session_state.is_initialized = False
```

**ä¸ºä»€ä¹ˆéœ€è¦ session_stateï¼Ÿ**

Streamlit çš„æ‰§è¡Œæ¨¡å‹å¾ˆç‰¹æ®Šï¼š**æ¯æ¬¡ç”¨æˆ·äº¤äº’ï¼ˆç‚¹å‡»æŒ‰é’®ã€è¾“å…¥æ–‡å­—ï¼‰ï¼Œæ•´ä¸ª `app.py` è„šæœ¬ä¼šä»å¤´åˆ°å°¾é‡æ–°æ‰§è¡Œä¸€éã€‚** æ™®é€šçš„ Python å˜é‡æ¯æ¬¡é‡æ–°æ‰§è¡Œéƒ½ä¼šè¢«é‡ç½®ã€‚`st.session_state` æ˜¯ Streamlit æä¾›çš„æŒä¹…åŒ–å­˜å‚¨â€”â€”è·¨æ¬¡è¿è¡Œä¿ç•™æ•°æ®ã€‚

| çŠ¶æ€å˜é‡ | ç±»å‹ | ç”¨é€” |
|----------|------|------|
| `chat_history` | `List[dict]` | å­˜å‚¨æ‰€æœ‰èŠå¤©æ¶ˆæ¯ï¼ˆç”¨æˆ·+AIï¼‰ï¼Œç”¨äºæ¸²æŸ“èŠå¤©ç•Œé¢ |
| `rag_pipeline` | `RAGPipeline` or `None` | RAG æµæ°´çº¿å®ä¾‹ï¼ˆå†…éƒ¨åŒ…å«å‘é‡åº“ï¼‰ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°åˆ›å»º |
| `is_initialized` | `bool` | æ ‡è®°ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åŠ è½½ |

**ä¸¾ä¾‹è¯´æ˜ Streamlit çš„é‡æ–°æ‰§è¡Œæœºåˆ¶ï¼š**

```
ç”¨æˆ·ç¬¬1æ¬¡æ‰“å¼€é¡µé¢:
  app.py æ‰§è¡Œ â†’ session_state ä¸ºç©º â†’ åˆå§‹åŒ–æ‰€æœ‰å˜é‡ â†’ åŠ è½½å‘é‡åº“ â†’ æ¸²æŸ“ç©ºèŠå¤©ç•Œé¢

ç”¨æˆ·è¾“å…¥ "What is BMP15?" å¹¶æŒ‰å›è½¦:
  app.py ä»å¤´æ‰§è¡Œ â†’ session_state å·²æœ‰æ•°æ® â†’ è·³è¿‡åˆå§‹åŒ– â†’ æ¸²æŸ“ä¹‹å‰çš„èŠå¤© â†’ å¤„ç†æ–°é—®é¢˜

ç”¨æˆ·å†è¾“å…¥ "How does it work?":
  app.py ä»å¤´æ‰§è¡Œ â†’ session_state æœ‰ 2 æ¡æ¶ˆæ¯ â†’ æ¸²æŸ“ä¹‹å‰çš„èŠå¤© â†’ å¤„ç†æ–°é—®é¢˜
```

å¦‚æœä¸ç”¨ `session_state`ï¼Œæ¯æ¬¡ç”¨æˆ·è¾“å…¥åï¼Œå¯¹è¯å†å²å’Œ RAG æµæ°´çº¿éƒ½ä¼šè¢«æ¸…ç©ºï¼Œå¤šè½®å¯¹è¯å°±ä¸å¯èƒ½å®ç°ã€‚

#### æ®µè½ 3: ä¾§è¾¹æ å’Œç³»ç»Ÿåˆå§‹åŒ–ï¼ˆç¬¬ 55-100 è¡Œï¼‰

```python
# ä¾§è¾¹æ 
with st.sidebar:
    st.title("ğŸ§¬ Oocyte Expert")
    st.markdown("""...""")

    if st.session_state.is_initialized:
        st.success("Knowledge Base: Active âœ…")
    else:
        st.warning("Knowledge Base: Loading...")

    if st.button("Reset System"):
        st.session_state.chat_history = []
        st.experimental_rerun()

# ç³»ç»Ÿåˆå§‹åŒ–ï¼ˆåªåœ¨é¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
if not st.session_state.is_initialized:
    with st.spinner("Initializing knowledge base..."):
        try:
            vector_store_manager = VectorStoreManager()
            try:
                vector_store = vector_store_manager.load_vector_store("data/chroma_db")
            except ValueError:
                st.error("Vector store not found. Please process PDF documents first.")
                st.stop()

            st.session_state.rag_pipeline = RAGPipeline(vector_store)
            st.session_state.is_initialized = True
        except Exception as e:
            st.error(f"Error initializing system: {str(e)}")
            st.stop()
```

**ä¾§è¾¹æ åšäº†ä»€ä¹ˆï¼š**
- å±•ç¤ºé¡¹ç›®æ ‡é¢˜å’Œç®€ä»‹
- ç”¨ `st.success` / `st.warning` æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€ï¼ˆç»¿è‰² = å°±ç»ªï¼Œé»„è‰² = åŠ è½½ä¸­ï¼‰
- "Reset System" æŒ‰é’®æ¸…ç©ºå¯¹è¯å†å²å¹¶åˆ·æ–°é¡µé¢

**åˆå§‹åŒ–æµç¨‹ï¼š**

```
is_initialized == False? (é¦–æ¬¡è®¿é—®)
        â†“ Yes
æ˜¾ç¤º spinner "Initializing knowledge base..."
        â†“
VectorStoreManager() â†’ åˆå§‹åŒ– Embedding æ¨¡å‹
        â†“
load_vector_store("data/chroma_db") â†’ ä»ç£ç›˜åŠ è½½å‘é‡åº“
        â†“ å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ â†’ æŠ¥é”™ "Please process PDF documents first." â†’ åœæ­¢
        â†“
RAGPipeline(vector_store) â†’ å¤ç”¨å·²åŠ è½½çš„å‘é‡åº“ï¼Œåˆ›å»º RAG æµæ°´çº¿
        â†“
is_initialized = True â†’ ä¸‹æ¬¡è„šæœ¬é‡æ–°æ‰§è¡Œæ—¶è·³è¿‡è¿™ä¸ªå—
```

**`st.stop()` çš„ä½œç”¨ï¼š** ç«‹å³åœæ­¢è„šæœ¬æ‰§è¡Œï¼Œé¡µé¢åªæ˜¾ç¤ºåˆ°ç›®å‰ä¸ºæ­¢æ¸²æŸ“çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ç§ä¼˜é›…çš„é”™è¯¯å¤„ç†â€”â€”å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œä¸è¦ç»§ç»­æ¸²æŸ“èŠå¤©ç•Œé¢ã€‚

#### æ®µè½ 4: èŠå¤©ç•Œé¢ï¼ˆç¬¬ 103-152 è¡Œï¼‰

```python
# æ¸²æŸ“å†å²æ¶ˆæ¯
for idx, message in enumerate(st.session_state.chat_history):
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "citations" in message and message["citations"]:
            with st.expander("View Citations"):
                for citation in message["citations"]:
                    st.markdown(f"*{citation}*")

# å¤„ç†æ–°è¾“å…¥
if prompt := st.chat_input("Ask your question about oocyte research..."):
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        if not st.session_state.rag_pipeline:
            st.error("System not initialized. Please wait...")
        else:
            with st.spinner("Researching..."):
                try:
                    response = st.session_state.rag_pipeline.ask(prompt)
                    st.session_state.chat_history.append({
                        "role": "assistant",
                        "content": response,
                        "citations": ["More detailed citations will be implemented"]
                    })
                    st.write(response)
                except Exception as e:
                    st.error(f"Error generating response: {str(e)}")

# åº•éƒ¨æŒ‰é’®
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if st.button("Clear Conversation"):
        st.session_state.chat_history = []
        st.experimental_rerun()
with col2:
    if st.button("Export Chat"):
        st.info("Export feature coming soon!")
```

**èŠå¤©æ¸²æŸ“æµç¨‹ï¼š**

```
ç¬¬1æ­¥: éå† chat_historyï¼Œæ¸²æŸ“ä¹‹å‰æ‰€æœ‰çš„å¯¹è¯æ¶ˆæ¯
       æ¯æ¡æ¶ˆæ¯ç”¨ st.chat_message() æ˜¾ç¤ºå¯¹åº”çš„å¤´åƒï¼ˆuser/assistantï¼‰
       å¦‚æœæ¶ˆæ¯æœ‰ citationsï¼Œç”¨ st.expander æŠ˜å å±•ç¤º
       â†“
ç¬¬2æ­¥: st.chat_input() æ˜¾ç¤ºè¾“å…¥æ¡†ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥
       â†“ ç”¨æˆ·è¾“å…¥ "What is BMP15?" å¹¶æŒ‰å›è½¦
       â†“
ç¬¬3æ­¥: æŠŠç”¨æˆ·æ¶ˆæ¯è¿½åŠ åˆ° chat_history
       â†“
ç¬¬4æ­¥: è°ƒç”¨ rag_pipeline.ask(prompt)
       â†’ Question Condensing â†’ Retrieval â†’ Generation
       â†“
ç¬¬5æ­¥: æŠŠ AI å›å¤è¿½åŠ åˆ° chat_historyï¼ˆå¸¦ citations å ä½ç¬¦ï¼‰
       â†“
ç¬¬6æ­¥: st.write(response) æ˜¾ç¤ºå›ç­”
       â†“
ç¬¬7æ­¥: è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œé¡µé¢æ›´æ–°ï¼Œæ˜¾ç¤ºå®Œæ•´å¯¹è¯
```

**`if prompt := st.chat_input(...)` æ˜¯ä»€ä¹ˆè¯­æ³•ï¼Ÿ**

è¿™æ˜¯ Python 3.8 çš„ **walrus operator** (`:=`)ã€‚å®ƒåŒæ—¶åšäº†èµ‹å€¼å’Œæ¡ä»¶æ£€æŸ¥ï¼š
- å¦‚æœç”¨æˆ·è¾“å…¥äº†å†…å®¹ï¼Œ`prompt` è¢«èµ‹å€¼ä¸ºè¾“å…¥çš„å­—ç¬¦ä¸²ï¼Œæ¡ä»¶ä¸º `True`ï¼Œè¿›å…¥ if å—
- å¦‚æœç”¨æˆ·æ²¡è¾“å…¥ï¼ˆé¡µé¢åˆšåŠ è½½ï¼‰ï¼Œ`prompt` ä¸º `None`ï¼Œæ¡ä»¶ä¸º `False`ï¼Œè·³è¿‡

**åº•éƒ¨æŒ‰é’®ï¼š**
- **Clear Conversationï¼š** æ¸…ç©º `chat_history` å¹¶åˆ·æ–°é¡µé¢ã€‚æ³¨æ„ä¹Ÿä½¿ç”¨ `st.experimental_rerun()` å¼ºåˆ¶é‡æ–°æ‰§è¡Œè„šæœ¬ï¼Œè¿™æ ·ç•Œé¢ç«‹åˆ»æ›´æ–°ã€‚
- **Export Chatï¼š** å ä½åŠŸèƒ½ï¼Œç›®å‰åªæ˜¾ç¤º "coming soon" æç¤ºã€‚

### é¢è¯•é—®ç­”

---

**Q: Explain how Streamlit's execution model works and how it affects your architecture.**

> "Streamlit has a unique execution model: every time a user interacts with the app â€” clicks a button, types in the chat input, toggles a checkbox â€” the entire Python script re-executes from top to bottom. This means any regular Python variable is reset on every interaction. That's why `st.session_state` is essential â€” it's a persistent key-value store that survives across re-executions within the same browser session.
>
> This model has architectural implications. Heavy initialization â€” like loading the vector store and creating the RAG pipeline â€” must be gated behind an `is_initialized` flag in session_state, otherwise it would re-run on every keystroke. The chat history must also live in session_state so previous messages aren't lost. And the RAG pipeline itself â€” including its conversation memory â€” must be stored in session_state so multi-turn context is preserved.
>
> The benefit of this model is simplicity: the script reads linearly, top to bottom, like a page layout. The trade-off is that you have to be explicit about what state persists and what doesn't."

---

**Q: How does the citation feature work?**

> "Currently, the citation implementation is a placeholder â€” you can see `citations: ['More detailed citations will be implemented']` in the code. However, the infrastructure for real citations is already in place. The `RAGPipeline.ask()` method returns a `source_documents` list in its response, where each document carries `metadata` with the source PDF filename and page number. To implement full citations, I'd extract that metadata and display it in the `st.expander` component, like: 'Source: s41467-021-21246-9.pdf, Page 5.' The UI component â€” the expandable citation panel â€” is already built; it just needs to be connected to the actual source_documents data."

---

**Q: What happens if the vector store doesn't exist when the app starts?**

> "The app handles this gracefully through defensive error handling. In the initialization block, it tries to call `load_vector_store('data/chroma_db')`. If that directory doesn't exist, the method raises a `ValueError`, which the app catches and displays as `st.error('Vector store not found. Please process PDF documents first.')`, followed by `st.stop()` which halts the script. The user sees a clear error message telling them to run `process_pdfs.py` first. The app doesn't crash â€” it just stops rendering the chat interface since there's no knowledge base to query."

---

## 7. `requirements.txt`

```
streamlit==1.31.1
langchain==0.1.0
langchain-community==0.0.13
langchain-openai==0.0.2
openai==1.60.0
chromadb==0.3.29
python-dotenv==1.0.0
pypdf2==3.0.1
tiktoken==0.5.2
```

æ¯ä¸ªä¾èµ–çš„ä½œç”¨ï¼š

| åŒ… | ç‰ˆæœ¬ | ç”¨åœ¨å“ªé‡Œ | åšä»€ä¹ˆ |
|----|------|---------|--------|
| `streamlit` | 1.31.1 | `app.py` | Web UI æ¡†æ¶ï¼Œæä¾› chat ç»„ä»¶ã€session stateã€éƒ¨ç½² |
| `langchain` | 0.1.0 | å…¨éƒ¨ `src/` | LLM åº”ç”¨ç¼–æ’æ¡†æ¶ï¼Œæä¾› Chainã€Memoryã€TextSplitter |
| `langchain-community` | 0.0.13 | `src/` | LangChain ç¤¾åŒºé›†æˆåŒ…ï¼Œæä¾› PyPDFLoaderã€ChatOpenAIã€Chroma |
| `langchain-openai` | 0.0.2 | `src/` | LangChain çš„ OpenAI ä¸“ç”¨é›†æˆï¼Œæä¾› OpenAIEmbeddings |
| `openai` | 1.60.0 | (é—´æ¥) | OpenAI Python SDKï¼Œlangchain-openai åº•å±‚ä¾èµ– |
| `chromadb` | 0.3.29 | `src/embeddings.py` | å‘é‡æ•°æ®åº“ï¼Œå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£åµŒå…¥ |
| `python-dotenv` | 1.0.0 | `src/embeddings.py` | ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆAPI Keyï¼‰ |
| `pypdf2` | 3.0.1 | (é—´æ¥) | PyPDFLoader åº•å±‚ä¾èµ–ï¼Œè´Ÿè´£ PDF æ–‡ä»¶è§£æ |
| `tiktoken` | 0.5.2 | (é—´æ¥) | OpenAI çš„ tokenizerï¼ŒLangChain ç”¨å®ƒæ¥è®¡ç®— token æ•° |

### é¢è¯•é—®ç­”

---

**Q: Why did you pin specific versions in requirements.txt?**

> "Version pinning ensures reproducibility. LangChain in particular was evolving rapidly during this period â€” breaking changes between minor versions were common. If I specified `langchain>=0.1.0`, someone installing the project six months later might get version 0.3.0, which could have completely different APIs. By pinning `langchain==0.1.0`, I guarantee that anyone cloning the repo gets exactly the same behavior I tested against. In a production environment, I'd also use a lock file â€” like pip-compile or Poetry â€” to pin transitive dependencies as well."

---

## 8. ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“

æŠŠæ‰€æœ‰æ–‡ä»¶ä¸²èµ·æ¥ï¼Œçœ‹ä¸€ä¸ªå®Œæ•´çš„ç”¨æˆ·æ—…ç¨‹ï¼š

### æ—…ç¨‹ 1: ç®¡ç†å‘˜å‡†å¤‡çŸ¥è¯†åº“ï¼ˆè·‘ä¸€æ¬¡ï¼‰

```
ç®¡ç†å‘˜æŠŠ 3 ç¯‡è®ºæ–‡æ”¾è¿› data/pdfs/
         â†“
è¿è¡Œ python process_pdfs.py
         â†“
process_pdfs.py:
  â”‚
  â”œâ”€ DocumentProcessor.__init__()
  â”‚    â””â”€ åˆ›å»º TextSplitter(chunk_size=1000, overlap=200)
  â”‚
  â”œâ”€ DocumentProcessor.load_pdfs("data/pdfs")
  â”‚    â”œâ”€ DirectoryLoader æ‰¾åˆ° 3 ä¸ª PDF
  â”‚    â”œâ”€ PyPDFLoader æå–æ¯é¡µæ–‡æœ¬ â†’ çº¦ 30 ä¸ª Document
  â”‚    â””â”€ TextSplitter åˆ‡åˆ† â†’ çº¦ 100 ä¸ª Document å—
  â”‚
  â”œâ”€ VectorStoreManager.__init__()
  â”‚    â”œâ”€ load_dotenv() è¯»å– .env æ–‡ä»¶
  â”‚    â””â”€ OpenAIEmbeddings() åˆå§‹åŒ– embedding æ¨¡å‹
  â”‚
  â””â”€ VectorStoreManager.create_vector_store(100ä¸ªå—)
       â”œâ”€ Chroma.from_documents():
       â”‚    â”œâ”€ å¯¹ 100 ä¸ªå—é€ä¸ªè°ƒç”¨ OpenAI Embedding API
       â”‚    â”‚    æ¯ä¸ªå— â†’ 1536 ç»´å‘é‡
       â”‚    â””â”€ å­˜å…¥ ChromaDB å†…å­˜ç´¢å¼•
       â”œâ”€ vector_store.persist()
       â”‚    â””â”€ å†™å…¥ data/chroma_db/ ç›®å½•ï¼ˆSQLite + ç´¢å¼•æ–‡ä»¶ï¼‰
       â””â”€ æ‰“å° "Vector store created successfully!"
```

### æ—…ç¨‹ 2: ç”¨æˆ·æé—®ï¼ˆæ¯æ¬¡äº¤äº’ï¼‰

```
ç”¨æˆ·æ‰“å¼€æµè§ˆå™¨è®¿é—® Streamlit åº”ç”¨
         â†“
app.py é¦–æ¬¡æ‰§è¡Œ:
  â”œâ”€ session_state åˆå§‹åŒ–ï¼ˆç©ºåˆ—è¡¨ã€Noneã€Falseï¼‰
  â”œâ”€ VectorStoreManager().load_vector_store("data/chroma_db")
  â”‚    â””â”€ ä»ç£ç›˜åŠ è½½å‘é‡åº“ï¼ˆä¸è°ƒ APIï¼Œå¾ˆå¿«ï¼‰
  â”œâ”€ RAGPipeline(vector_store) â€” å¤ç”¨å·²åŠ è½½çš„å‘é‡åº“å®ä¾‹
  â”‚    â”œâ”€ self.vector_store = vector_store â€” ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å®ä¾‹
  â”‚    â”œâ”€ ConversationBufferMemory() â€” ç©ºçš„å¯¹è¯è®°å¿†
  â”‚    â””â”€ ConversationalRetrievalChain â€” ç»„è£…å®Œæ•´é“¾
  â””â”€ is_initialized = True
         â†“
ç”¨æˆ·è¾“å…¥: "What pathways regulate oocyte maturation?"
         â†“
app.py é‡æ–°æ‰§è¡Œ:
  â”œâ”€ is_initialized == True â†’ è·³è¿‡åˆå§‹åŒ–
  â”œâ”€ æ¸²æŸ“ç©ºçš„èŠå¤©ç•Œé¢
  â”œâ”€ st.chat_input æ•è·ç”¨æˆ·è¾“å…¥
  â”œâ”€ è¿½åŠ ç”¨æˆ·æ¶ˆæ¯åˆ° chat_history
  â””â”€ rag_pipeline.ask("What pathways regulate oocyte maturation?")
       â”‚
       â”œâ”€ ConversationalRetrievalChain æ‰§è¡Œ:
       â”‚    â”‚
       â”‚    â”œâ”€ Step 1: Question Condensing
       â”‚    â”‚    chat_history ä¸ºç©º â†’ ç›´æ¥ç”¨åŸé—®é¢˜
       â”‚    â”‚
       â”‚    â”œâ”€ Step 2: Retrieval
       â”‚    â”‚    â”œâ”€ OpenAI Embedding API: é—®é¢˜ â†’ 1536 ç»´å‘é‡
       â”‚    â”‚    â”œâ”€ ChromaDB: ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ top-4
       â”‚    â”‚    â””â”€ è¿”å› 4 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
       â”‚    â”‚
       â”‚    â”œâ”€ Step 3: Generation
       â”‚    â”‚    â”œâ”€ Prompt = "Based on context: [4ä¸ªå—], answer: [é—®é¢˜]"
       â”‚    â”‚    â””â”€ GPT-3.5-turbo ç”Ÿæˆå›ç­”
       â”‚    â”‚
       â”‚    â””â”€ Step 4: Memory Update
       â”‚         â””â”€ å­˜å‚¨ Q&A å¯¹åˆ° chat_history
       â”‚
       â””â”€ è¿”å› {"answer": "Several pathways...", "source_documents": [...]}
         â†“
st.write(response) â†’ åœ¨ç•Œé¢æ˜¾ç¤ºå›ç­”
è¿½åŠ  AI æ¶ˆæ¯åˆ° chat_history
         â†“
ç”¨æˆ·æ¥ç€é—®: "Are any of those druggable?"
         â†“
app.py é‡æ–°æ‰§è¡Œ:
  â”œâ”€ æ¸²æŸ“ä¹‹å‰çš„ 2 æ¡æ¶ˆæ¯
  â””â”€ rag_pipeline.ask("Are any of those druggable?")
       â”‚
       â””â”€ Step 1: Question Condensing
            â”œâ”€ chat_history = [ä¹‹å‰çš„ Q&A]
            â”œâ”€ LLM é‡å†™: "Are any pathways that regulate oocyte maturation druggable?"
            â””â”€ ç”¨é‡å†™åçš„é—®é¢˜å»æ£€ç´¢ â†’ ç”Ÿæˆ â†’ è¿”å›å›ç­”
```

### é¢è¯•ç»ˆæé—®ç­”

---

**Q: If I gave you 10 minutes to demo this system to a pharma customer, how would you structure it?**

> "I'd structure the 10 minutes into three acts. Act one â€” the problem, 2 minutes: 'Your team published three papers on oocyte biology. You want to ask questions across all of them simultaneously, with citations. Today that requires reading each paper end to end.' Act two â€” the solution, 6 minutes: live demo. I'd ask a domain-specific question like 'What signaling pathways regulate oocyte maturation?' and show the system returning an evidence-based answer with source citations. Then I'd ask a follow-up â€” 'Are any of those pathways druggable?' â€” to demonstrate multi-turn context. I'd point out that the system remembered what 'those' refers to. Act three â€” the bridge, 2 minutes: 'This is what I built with three papers and vector search. Imagine this at the scale of QIAGEN's Biomedical Knowledge Base â€” millions of curated findings, explicit pathway relationships, and graph-based reasoning. That's the step function improvement we can offer your team.'"

---

**Q: What's the single biggest weakness of this system?**

> "The system retrieves text chunks based on semantic similarity but has no understanding of biological entity relationships. If I ask 'What is upstream of SMAD signaling in oocyte maturation?', the system can only find chunks that happen to mention both 'upstream' and 'SMAD' â€” it doesn't actually traverse a signaling pathway. A knowledge graph would let me do exactly that: start at the SMAD node, follow 'activated_by' edges, and return the upstream regulators. That's the fundamental difference between what BioRAG does with vector search and what QIAGEN does with a curated biomedical knowledge graph. My project is a proof of concept for one half of the equation. QIAGEN has the other half â€” and the combination of both is where the real value lies."

---

> **å¤ä¹ å»ºè®®ï¼š** é¢è¯•å‰æŠŠè¿™ä»½æ–‡æ¡£è¯»ä¸¤éã€‚ç¬¬ä¸€éè¯»è§£é‡Šï¼Œç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ã€‚ç¬¬äºŒéåªè¯»é¢è¯•é—®ç­”éƒ¨åˆ†ï¼Œç»ƒä¹ ç”¨è‹±æ–‡å›ç­”ã€‚é‡ç‚¹è®°ä½ ConversationalRetrievalChain çš„ä¸‰æ­¥å†…éƒ¨æµç¨‹ï¼ˆquestion condensing â†’ retrieval â†’ generationï¼‰ï¼Œè¿™æ˜¯æœ€é«˜é¢‘çš„æŠ€æœ¯é¢è¯•é¢˜ã€‚
