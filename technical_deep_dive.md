# BioRAG ç³»ç»ŸæŠ€æœ¯æ·±åº¦æ‹†è§£ â€” é€æ–‡ä»¶ã€é€å‡½æ•°ã€é€è¡Œè§£è¯»

> è¿™ä»½æ–‡æ¡£çš„ç›®çš„ï¼šè®©ä½ åœ¨é¢è¯•å‰å½»åº•ææ‡‚è‡ªå·±å†™çš„æ¯ä¸€è¡Œä»£ç ã€‚æ¯ä¸ªæ–‡ä»¶ã€æ¯ä¸ªç±»ã€æ¯ä¸ªå‡½æ•°éƒ½ä¼šè®²æ¸…æ¥š **åšäº†ä»€ä¹ˆ â†’ ä¸ºä»€ä¹ˆè¿™ä¹ˆåš â†’ è¾“å…¥è¾“å‡ºæ˜¯ä»€ä¹ˆ â†’ é¢è¯•å®˜ä¼šæ€ä¹ˆé—® â†’ ä½ åº”è¯¥æ€ä¹ˆå›ç­”**ã€‚

---

## ç›®å½•

1. [ç³»ç»Ÿå…¨æ™¯å›¾ â€” æ•°æ®æ€ä¹ˆä» PDF å˜æˆå›ç­”](#1-ç³»ç»Ÿå…¨æ™¯å›¾)
2. [æ–‡ä»¶ 1: `src/document_loader.py` â€” æ–‡æ¡£åŠ è½½ä¸åˆ†å—](#2-document_loaderpy)
3. [æ–‡ä»¶ 2: `src/embeddings.py` â€” å‘é‡å­˜å‚¨ç®¡ç†](#3-embeddingspy)
4. [æ–‡ä»¶ 3: `src/rag_pipeline.py` â€” RAG æ ¸å¿ƒæµæ°´çº¿](#4-rag_pipelinepy)
5. [æ–‡ä»¶ 4: `process_pdfs.py` â€” ç¦»çº¿é¢„å¤„ç†è„šæœ¬](#5-process_pdfspy)
6. [æ–‡ä»¶ 5: `app.py` â€” Streamlit Web åº”ç”¨](#6-apppy)
7. [æ–‡ä»¶ 6: `requirements.txt` â€” ä¾èµ–æ¸…å•](#7-requirementstxt)
8. [ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“](#8-ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“)

---

## 1. ç³»ç»Ÿå…¨æ™¯å›¾

å…ˆçœ‹å¤§å›¾ï¼Œä½ çš„ç³»ç»Ÿåˆ†ä¸¤ä¸ªé˜¶æ®µè¿è¡Œï¼š

```
é˜¶æ®µä¸€ï¼šç¦»çº¿é¢„å¤„ç†ï¼ˆåªè·‘ä¸€æ¬¡ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF è®ºæ–‡    â”‚ â†’  â”‚ DocumentProcessor â”‚ â†’  â”‚ VectorStoreManagerâ”‚ â†’  â”‚  ChromaDB    â”‚
â”‚  (data/papers) â”‚    â”‚ (åŠ è½½+åˆ†å—)       â”‚    â”‚ (ç¼–ç +å­˜å‚¨)        â”‚    â”‚ (æŒä¹…åŒ–å‘é‡åº“) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     process_pdfs.py é©±åŠ¨è¿™ä¸ªæµç¨‹

é˜¶æ®µäºŒï¼šåœ¨çº¿é—®ç­”ï¼ˆç”¨æˆ·æ¯æ¬¡æé—®ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·é—®é¢˜    â”‚ â†’  â”‚  ChromaDB    â”‚ â†’  â”‚ ConversationalRetrieval  â”‚ â†’  â”‚  å›ç­”+å¼•ç”¨  â”‚
â”‚  (è‡ªç„¶è¯­è¨€)  â”‚    â”‚ (æ£€ç´¢top-4)  â”‚    â”‚ Chain (GPT-3.5-turbo)    â”‚    â”‚  (å±•ç¤ºåœ¨UI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     app.py + rag_pipeline.py é©±åŠ¨è¿™ä¸ªæµç¨‹
```

é¢è¯•æ—¶è®°ä½è¿™ä¸ªä¸¤é˜¶æ®µæ¶æ„ã€‚é¢è¯•å®˜é—®"walk me through your system"ï¼Œå°±æŒ‰è¿™ä¸ªé¡ºåºè®²ã€‚

---

## 2. `document_loader.py`

è¿™æ˜¯ç³»ç»Ÿçš„**ç¬¬ä¸€ç«™**â€”â€”æŠŠ PDF æ–‡ä»¶å˜æˆ LLM èƒ½å¤„ç†çš„æ–‡æœ¬å—ã€‚

### å®Œæ•´æºç ï¼ˆ21 è¡Œï¼‰

```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )

    def load_pdfs(self, directory_path):
        """Load all PDFs from specified directory"""
        loader = DirectoryLoader(
            directory_path,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        documents = loader.load()
        return self.text_splitter.split_documents(documents)
```

### é€å—æ‹†è§£

#### Import éƒ¨åˆ†

```python
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
```

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `PyPDFLoader` | è¯»å–å•ä¸ª PDF æ–‡ä»¶ï¼Œæå–æ¯ä¸€é¡µçš„çº¯æ–‡æœ¬ |
| `DirectoryLoader` | æ‰«æä¸€ä¸ªç›®å½•ï¼Œæ‰¹é‡åŠ è½½æ‰€æœ‰åŒ¹é…æ–‡ä»¶ |
| `RecursiveCharacterTextSplitter` | æŠŠé•¿æ–‡æœ¬åˆ‡æˆå°å—ï¼Œæ˜¯ LangChain æœ€å¸¸ç”¨çš„åˆ†å—å™¨ |

#### `__init__` æ–¹æ³•

```python
def __init__(self):
    self.text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
```

**åšäº†ä»€ä¹ˆï¼š** åˆ›å»ºä¸€ä¸ªæ–‡æœ¬åˆ†å‰²å™¨å®ä¾‹ï¼Œé…ç½®å¥½åˆ†å—å‚æ•°ã€‚

**ä¸‰ä¸ªå‚æ•°çš„å«ä¹‰ï¼š**

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|----|------|
| `chunk_size` | 1000 | æ¯ä¸ªå—æœ€å¤š 1000 ä¸ªå­—ç¬¦ï¼ˆå¤§çº¦ 150-200 ä¸ªè‹±æ–‡å•è¯ï¼Œç›¸å½“äºä¸€ä¸ªè‡ªç„¶æ®µï¼‰ |
| `chunk_overlap` | 200 | ç›¸é‚»ä¸¤ä¸ªå—ä¹‹é—´æœ‰ 200 å­—ç¬¦çš„é‡å  |
| `length_function` | `len` | ç”¨ Python å†…ç½®çš„ `len()` æ¥è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼ˆæŒ‰å­—ç¬¦æ•°ï¼‰ |

**ä¸ºä»€ä¹ˆè¦é‡å ï¼ˆoverlapï¼‰ï¼Ÿ** ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾åŸæ–‡æ˜¯ï¼š
> "BMP15 is a key growth factor. It activates the SMAD signaling pathway, which regulates oocyte maturation."

å¦‚æœåœ¨ "pathway" ä¹‹ååˆšå¥½åˆ‡æ–­ï¼Œæ²¡æœ‰é‡å çš„è¯ï¼š
- å— A: "...It activates the SMAD signaling pathway,"
- å— B: "which regulates oocyte maturation."

å— B å­¤ç«‹åœ°çœ‹ï¼Œä½ ä¸çŸ¥é“"which"æŒ‡çš„æ˜¯ä»€ä¹ˆã€‚æœ‰äº† 200 å­—ç¬¦çš„é‡å ï¼š
- å— A: "...It activates the SMAD signaling pathway, which regulates oocyte maturation."
- å— B: "...SMAD signaling pathway, which regulates oocyte maturation. [åç»­å†…å®¹]..."

è¿™æ · BMP15 â†’ SMAD â†’ oocyte maturation çš„å…³ç³»åœ¨è‡³å°‘ä¸€ä¸ªå—ä¸­æ˜¯å®Œæ•´çš„ã€‚

**`RecursiveCharacterTextSplitter` çš„"é€’å½’"æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ** å®ƒä¼šæŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§åˆ†éš”ç¬¦æ¥åˆ‡å‰²ï¼š
1. å…ˆå°è¯• `\n\n`ï¼ˆæ®µè½åˆ†éš”ï¼‰
2. å†å°è¯• `\n`ï¼ˆæ¢è¡Œï¼‰
3. å†å°è¯• ` `ï¼ˆç©ºæ ¼ï¼‰
4. æœ€åé€å­—ç¬¦åˆ‡å‰²

è¿™æ ·å°½é‡åœ¨è¯­ä¹‰è‡ªç„¶çš„åœ°æ–¹æ–­å¼€ï¼Œè€Œä¸æ˜¯åœ¨å•è¯ä¸­é—´ç¡¬åˆ‡ã€‚

#### `load_pdfs` æ–¹æ³•

```python
def load_pdfs(self, directory_path):
    loader = DirectoryLoader(
        directory_path,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader
    )
    documents = loader.load()
    return self.text_splitter.split_documents(documents)
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | ä¾‹å­ |
|------|------|------|
| `directory_path` | `str` | `"data/papers"` |

**å†…éƒ¨æµç¨‹ï¼š**

```
ç¬¬1æ­¥: DirectoryLoader æ‰«æ data/papers/ ä¸‹æ‰€æœ‰ .pdf æ–‡ä»¶
       â†“
ç¬¬2æ­¥: å¯¹æ¯ä¸ª PDF ç”¨ PyPDFLoader é€é¡µæå–æ–‡æœ¬
       â†“ å¾—åˆ° List[Document]ï¼Œæ¯ä¸ª Document = ä¸€é¡µ PDF
       â†“ Document å¯¹è±¡æœ‰ä¸¤ä¸ªå±æ€§ï¼š
       â†“   .page_content = "è¿™ä¸€é¡µçš„æ–‡æœ¬å†…å®¹"
       â†“   .metadata = {"source": "data/papers/paper1.pdf", "page": 0}
       â†“
ç¬¬3æ­¥: text_splitter.split_documents() æŠŠæ¯é¡µæ–‡æœ¬åˆ‡æˆ ~1000 å­—ç¬¦çš„å—
       â†“ metadata ä¼šè¢«ç»§æ‰¿ï¼Œæ¯ä¸ªå—éƒ½çŸ¥é“è‡ªå·±æ¥è‡ªå“ªä¸ªæ–‡ä»¶ã€å“ªä¸€é¡µ
```

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| æ–‡æ¡£å—åˆ—è¡¨ | `List[Document]` | æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª ~1000 å­—ç¬¦çš„æ–‡æœ¬å—ï¼Œå¸¦æœ‰ source å’Œ page å…ƒæ•°æ® |

**å…·ä½“ä¾‹å­ï¼š** å‡è®¾ä½ æœ‰ 3 ç¯‡è®ºæ–‡ï¼Œæ¯ç¯‡ 10 é¡µï¼Œæ¯é¡µçº¦ 3000 å­—ç¬¦ï¼š
- `loader.load()` è¿”å› 30 ä¸ª Documentï¼ˆ3 ç¯‡ Ã— 10 é¡µï¼‰
- `split_documents()` æŠŠæ¯é¡µåˆ‡æˆçº¦ 3-4 ä¸ªå—ï¼ˆ3000 Ã· 1000ï¼Œè€ƒè™‘é‡å ï¼‰
- æœ€ç»ˆè¿”å›çº¦ 90-120 ä¸ªå—

### é¢è¯•é—®ç­”

---

**Q: Why did you choose RecursiveCharacterTextSplitter over other splitters?**

> "RecursiveCharacterTextSplitter is the most commonly recommended splitter in LangChain for general-purpose text, and here's why. It attempts to split at semantically meaningful boundaries â€” paragraph breaks first, then sentence breaks, then word breaks â€” rather than just cutting at a fixed character count. For scientific papers, this is particularly important because a key finding like 'BMP15 activates SMAD signaling' should ideally stay within a single chunk. The recursive approach maximizes the chance of that happening.
>
> Alternatives I considered: CharacterTextSplitter just splits on a single separator, which is less flexible. TokenTextSplitter splits by token count, which is useful when you need precise token budgets for the LLM, but character-based splitting was sufficient for my use case. If I were handling more structured documents, I might use something like MarkdownTextSplitter or a custom splitter that respects section headers like Abstract, Methods, Results."

---

**Q: Your chunk_size is 1000 characters. How did you arrive at that number?**

> "It's a trade-off between three factors. First, retrieval precision: smaller chunks mean each chunk is about one specific idea, so cosine similarity search is more precise. But too small â€” say 200 characters â€” and you lose context. A chunk that says 'it was upregulated' is useless without knowing what 'it' refers to. Second, LLM context window: each retrieved chunk consumes tokens. I retrieve 4 chunks, so that's roughly 4000 characters or about 1000 tokens â€” well within GPT-3.5-turbo's 16K token window, leaving plenty of room for conversation history and the generated answer. Third, semantic completeness: 1000 characters is roughly one paragraph in a scientific paper, which typically contains one complete idea or finding.
>
> I experimented with 500 and 1500. At 500, too many retrieval results were fragmentary. At 1500, irrelevant information was getting mixed in with relevant content. 1000 was the sweet spot for this corpus."

---

**Q: What happens if a PDF has images, tables, or equations?**

> "PyPDFLoader only extracts text content â€” it cannot parse images, table structures, or mathematical equations. This is a known limitation. For scientific papers, this means experimental data in tables and method details in figures are lost. To address this, I'd consider Unstructured.io for table extraction, a multimodal model like GPT-4V for figure description, or LlamaParse which is specifically designed for RAG-friendly document parsing. But for my MVP, text-only extraction was sufficient to demonstrate the core RAG workflow."

---

## 3. `embeddings.py`

è¿™æ˜¯ç³»ç»Ÿçš„**ç¬¬äºŒç«™**â€”â€”æŠŠæ–‡æœ¬å—å˜æˆå‘é‡ï¼Œå­˜è¿›æ•°æ®åº“ã€‚

### å®Œæ•´æºç ï¼ˆ35 è¡Œï¼‰

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
import os

class VectorStoreManager:
    def __init__(self):
        load_dotenv()
        os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
        self.embeddings = OpenAIEmbeddings()

    def create_vector_store(self, documents, persist_directory="data/chroma_db"):
        """Create or update vector store"""
        if not os.path.exists(persist_directory):
            os.makedirs(persist_directory)

        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=persist_directory
        )
        return vector_store

    def load_vector_store(self, persist_directory="data/chroma_db"):
        """Load existing vector store"""
        if not os.path.exists(persist_directory):
            raise ValueError("Vector store not found!")

        return Chroma(
            embedding_function=self.embeddings,
            persist_directory=persist_directory
        )
```

### é€å—æ‹†è§£

#### `__init__` æ–¹æ³•

```python
def __init__(self):
    load_dotenv()                                              # 1
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") # 2
    self.embeddings = OpenAIEmbeddings()                       # 3
```

ä¸‰è¡Œä»£ç åšäº†ä¸‰ä»¶äº‹ï¼š

| è¡Œ | åšäº†ä»€ä¹ˆ | ä¸ºä»€ä¹ˆ |
|----|---------|--------|
| 1 | ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ | `.env` é‡Œå­˜äº† `OPENAI_API_KEY=sk-xxxxx`ï¼Œè¿™è¡ŒæŠŠå®ƒè¯»è¿›å†…å­˜ |
| 2 | æŠŠ API Key è®¾è¿› `os.environ` | ç¡®ä¿ OpenAI SDK èƒ½é€šè¿‡ç¯å¢ƒå˜é‡æ‰¾åˆ° Key |
| 3 | åˆ›å»º OpenAI Embedding å®ä¾‹ | é»˜è®¤ä½¿ç”¨ `text-embedding-ada-002` æ¨¡å‹ï¼Œè¾“å‡º 1536 ç»´å‘é‡ |

**å…³äº Embedding æ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚ï¼š**
- æ¨¡å‹åï¼š`text-embedding-ada-002`
- è¾“å‡ºç»´åº¦ï¼š1536
- åŸç†ï¼šæŠŠä»»æ„é•¿åº¦çš„æ–‡æœ¬æ˜ å°„åˆ°ä¸€ä¸ª 1536 ç»´çš„æµ®ç‚¹æ•°å‘é‡
- æ€§è´¨ï¼šè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ â†’ å‘é‡åœ¨ç©ºé—´ä¸­è·ç¦»è¿‘ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦é«˜ï¼‰

ä¸¾ä¾‹ï¼š
```
"oocyte maturation process"  â†’  [0.012, -0.034, 0.056, ..., 0.078]  (1536ä¸ªæ•°)
"egg cell development"       â†’  [0.011, -0.031, 0.058, ..., 0.075]  (å¾ˆæ¥è¿‘!)
"weather forecast tomorrow"  â†’  [0.892, 0.445, -0.223, ..., -0.567] (å¾ˆè¿œ!)
```

#### `create_vector_store` æ–¹æ³•

```python
def create_vector_store(self, documents, persist_directory="data/chroma_db"):
    if not os.path.exists(persist_directory):
        os.makedirs(persist_directory)

    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=self.embeddings,
        persist_directory=persist_directory
    )
    return vector_store
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `documents` | `List[Document]` | ä¸Šä¸€æ­¥ `DocumentProcessor.load_pdfs()` è¿”å›çš„æ–‡æœ¬å—åˆ—è¡¨ |
| `persist_directory` | `str` | å‘é‡åº“å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ `"data/chroma_db"` |

**å†…éƒ¨æµç¨‹ï¼š**

```
ç¬¬1æ­¥: æ£€æŸ¥å­˜å‚¨ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±åˆ›å»º
       â†“
ç¬¬2æ­¥: Chroma.from_documents() åšäº†ä¸¤ä»¶äº‹ï¼š
       a) å¯¹æ¯ä¸ª Document çš„ page_content è°ƒç”¨ OpenAI Embedding API
          "BMP15 activates SMAD..." â†’ [0.012, -0.034, ..., 0.078] (1536ç»´)
       b) æŠŠå‘é‡ + åŸæ–‡ + metadata å­˜è¿› ChromaDB
       ï¼ˆChromaDB 0.4+ åœ¨æŒ‡å®š persist_directory åä¼šè‡ªåŠ¨æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œæ— éœ€æ˜¾å¼è°ƒç”¨ persist()ï¼‰
       ç”Ÿæˆæ–‡ä»¶åœ¨ data/chroma_db/ ç›®å½•ä¸‹
```

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| `vector_store` | `Chroma` | å¯ä»¥ç›´æ¥ç”¨æ¥åšç›¸ä¼¼åº¦æœç´¢çš„å‘é‡åº“å¯¹è±¡ |

**å®é™…å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ** å‡è®¾ä½ æœ‰ 100 ä¸ªæ–‡æœ¬å—ï¼š
- å‘ OpenAI API å‘é€ 100 æ¬¡ Embedding è¯·æ±‚ï¼ˆæˆ–æ‰¹é‡å‘é€ï¼‰
- æ¯ä¸ªå—å˜æˆ 1536 ä¸ªæµ®ç‚¹æ•°
- 100 ä¸ªå‘é‡ + 100 æ®µåŸæ–‡ + 100 æ¡ metadata å­˜å…¥ ChromaDB
- ChromaDB 0.4+ åœ¨æŒ‡å®š `persist_directory` åè‡ªåŠ¨æŒä¹…åŒ–åˆ° `data/chroma_db/` ç›®å½•ï¼ˆSQLite + ç´¢å¼•æ–‡ä»¶ï¼‰ï¼Œæ— éœ€æ˜¾å¼è°ƒç”¨ `persist()`

#### `load_vector_store` æ–¹æ³•

```python
def load_vector_store(self, persist_directory="data/chroma_db"):
    if not os.path.exists(persist_directory):
        raise ValueError("Vector store not found!")

    return Chroma(
        embedding_function=self.embeddings,
        persist_directory=persist_directory
    )
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `persist_directory` | `str` | ä¹‹å‰åˆ›å»ºçš„å‘é‡åº“è·¯å¾„ |

**åšäº†ä»€ä¹ˆï¼š** ä»ç£ç›˜åŠ è½½å·²æœ‰çš„å‘é‡åº“ã€‚æ³¨æ„å®ƒä¸éœ€è¦é‡æ–°ç¼–ç æ–‡æ¡£â€”â€”å‘é‡å·²ç»å­˜å¥½äº†ã€‚å®ƒåªéœ€è¦ `embedding_function` æ˜¯å› ä¸ºåç»­æŸ¥è¯¢æ—¶éœ€è¦æŠŠç”¨æˆ·çš„é—®é¢˜ä¹Ÿç¼–ç æˆå‘é‡ã€‚

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| `Chroma` å¯¹è±¡ | `Chroma` | ä»ç£ç›˜æ¢å¤çš„å‘é‡åº“ï¼Œå¯ä»¥ç›´æ¥æœç´¢ |

**`create` vs `load` çš„åŒºåˆ«ï¼š**
- `create_vector_store`ï¼šç¬¬ä¸€æ¬¡è·‘ï¼Œä»é›¶å¼€å§‹ç¼–ç æ–‡æ¡£å¹¶å­˜å‚¨ã€‚æ…¢ï¼ˆè¦è°ƒ APIï¼‰ã€‚
- `load_vector_store`ï¼šåç»­æ¯æ¬¡å¯åŠ¨åº”ç”¨æ—¶ç”¨ï¼Œåªæ˜¯ä»ç£ç›˜è¯»å–å·²æœ‰æ•°æ®ã€‚å¿«ï¼ˆä¸è°ƒ APIï¼‰ã€‚

### é¢è¯•é—®ç­”

---

**Q: Why ChromaDB and not Pinecone, FAISS, or Weaviate?**

> "I chose ChromaDB for three reasons specific to this project's constraints. First, local persistence: ChromaDB stores data as files on disk â€” no need for a running server process or cloud service. For a prototype with three papers, this is ideal. Second, LangChain integration: ChromaDB has first-class support in LangChain â€” `Chroma.from_documents()` handles embedding and indexing in a single call. Third, zero infrastructure: no Docker, no API keys for the database, no cost.
>
> The trade-offs are clear. ChromaDB doesn't support horizontal scaling â€” if I had millions of vectors, I'd need Pinecone for managed scaling or Milvus for self-hosted distributed search. FAISS, from Facebook, would give me better raw search performance, but it doesn't have built-in persistence â€” I'd need to manage save/load logic myself. For this project's scope â€” thousands of vectors, single-user â€” ChromaDB was the right call."

---

**Q: What embedding model are you using, and what are its characteristics?**

> "I'm using OpenAI's text-embedding-ada-002, which outputs 1536-dimensional dense vectors. It's trained on a large corpus with a contrastive learning objective, meaning it learns to place semantically similar texts close together in vector space and dissimilar texts far apart. It supports up to 8191 tokens of input.
>
> Key characteristics: it's multilingual, so a query in English can match a passage about the same concept in Chinese. It produces normalized vectors, which means cosine similarity and dot product give the same ranking. And it's relatively cheap â€” about $0.0001 per 1000 tokens.
>
> If I were to improve this, I'd consider domain-specific embedding models like BiomedBERT or PubMedBERT embeddings, which are fine-tuned on biomedical literature and might better capture specialized terminology like gene names and pathway terms."

---

**Q: Why don't you call `persist()` after creating the vector store?**

> "Starting with ChromaDB 0.4+, explicit `persist()` calls are no longer needed â€” and in fact are deprecated. When you pass a `persist_directory` to `Chroma.from_documents()`, ChromaDB automatically persists data to disk on every write operation. Under the hood, it uses SQLite for metadata storage and a custom index format for the vectors. The directory contains files like `chroma.sqlite3` for metadata and index files for the vector index. This means if the process restarts, we can reload the exact same state without re-computing embeddings â€” which saves both time and API costs. The older pattern of calling `vector_store.persist()` manually was required in ChromaDB 0.3.x but is now unnecessary and will emit a deprecation warning."

---

## 4. `rag_pipeline.py`

è¿™æ˜¯ç³»ç»Ÿçš„**å¤§è„‘**â€”â€”æŠŠæ£€ç´¢å’Œç”Ÿæˆä¸²åœ¨ä¸€èµ·ï¼Œå®ç°å¯¹è¯å¼é—®ç­”ã€‚

### å®Œæ•´æºç 

```python
from langchain_openai import ChatOpenAI
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationalRetrievalChain

class RAGPipeline:
    def __init__(self, vector_store):
        # æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„å‘é‡åº“å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
        self.vector_store = vector_store

        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        self.qa_chain = ConversationalRetrievalChain.from_llm(
            ChatOpenAI(temperature=0),
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 4}),
            memory=self.memory,
            return_source_documents=True,
            output_key="answer",
            verbose=True
        )

    def ask(self, query: str):
        response = self.qa_chain({"question": query})
        return response

if __name__ == '__main__':
    from src.embeddings import VectorStoreManager
    vector_store_manager = VectorStoreManager()
    vector_store = vector_store_manager.load_vector_store()
    rag = RAGPipeline(vector_store)
    while True:
        user_input = input("Please type your question:")
        if user_input.lower() == 'exit':
            break
        result = rag.ask(user_input)
        print("Answerï¼š", result['answer'])
```

### é€å—æ‹†è§£

#### `__init__` æ–¹æ³• â€” ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„ç»„è£…

è¿™ä¸ª `__init__` ç»„è£…äº†æ•´ä¸ª RAG æµæ°´çº¿çš„ä¸‰ä¸ªå…³é”®é›¶ä»¶ï¼š

**é›¶ä»¶ 1: å‘é‡åº“ï¼ˆRetrieverï¼‰**
```python
self.vector_store = vector_store
```
æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„å‘é‡åº“å®ä¾‹ï¼ˆç”± `VectorStoreManager.load_vector_store()` åˆ›å»ºï¼‰ã€‚è¿™æ ·é¿å…äº†é‡å¤åˆ›å»º Chroma è¿æ¥å’Œ Embedding æ¨¡å‹ï¼Œå®ç°äº†èŒè´£åˆ†ç¦»ï¼š`VectorStoreManager` è´Ÿè´£ç®¡ç†å‘é‡åº“çš„åˆ›å»ºå’ŒåŠ è½½ï¼Œ`RAGPipeline` åªè´Ÿè´£é—®ç­”ã€‚

**é›¶ä»¶ 2: å¯¹è¯è®°å¿†**
```python
self.memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)
```

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|----|------|
| `memory_key` | `"chat_history"` | è®°å¿†å­˜å‚¨åœ¨å­—å…¸çš„ `chat_history` é”®ä¸‹ |
| `return_messages` | `True` | ä»¥ Message å¯¹è±¡åˆ—è¡¨çš„æ ¼å¼è¿”å›å†å²ï¼ˆè€Œä¸æ˜¯çº¯å­—ç¬¦ä¸²æ‹¼æ¥ï¼‰ |
| `output_key` | `"answer"` | æŒ‡å®šä» chain è¾“å‡ºä¸­å–å“ªä¸ª key ä½œä¸º AI å›å¤å­˜å…¥è®°å¿†ã€‚å› ä¸º chain è¿”å›å¤šä¸ª keyï¼ˆanswerã€source_documentsï¼‰ï¼Œä¸æŒ‡å®šä¼šæŠ¥é”™ |

**ConversationBufferMemory çš„å·¥ä½œæ–¹å¼ï¼š**

```
ç¬¬1è½®:
  ç”¨æˆ·: "What is BMP15?"
  AI: "BMP15 is a growth factor..."
  â†’ memory å­˜å‚¨: [HumanMessage("What is BMP15?"), AIMessage("BMP15 is a growth factor...")]

ç¬¬2è½®:
  ç”¨æˆ·: "How does it relate to oocyte maturation?"
  â†’ memory æŠŠä¹‹å‰çš„è®°å½•ä¸€èµ·å‘ç»™ LLMï¼Œè¿™æ · LLM çŸ¥é“ "it" = BMP15

ç¬¬3è½®:
  â†’ memory è¶Šæ¥è¶Šé•¿... è¿™å°±æ˜¯ BufferMemory çš„é—®é¢˜
```

**é›¶ä»¶ 3: å¯¹è¯æ£€ç´¢é“¾ï¼ˆæ ¸å¿ƒï¼ï¼‰**
```python
self.qa_chain = ConversationalRetrievalChain.from_llm(
    ChatOpenAI(temperature=0),                                      # LLM
    retriever=self.vector_store.as_retriever(search_kwargs={"k": 4}), # æ£€ç´¢å™¨
    memory=self.memory,                                              # è®°å¿†
    return_source_documents=True,                                     # è¿”å›å¼•ç”¨
    output_key="answer",                                              # è¾“å‡ºé”®å
    verbose=True                                                      # æ‰“å°è°ƒè¯•æ—¥å¿—
)
```

**æ¯ä¸ªå‚æ•°çš„ä½œç”¨ï¼š**

| å‚æ•° | å€¼ | ä½œç”¨ |
|------|----|------|
| `ChatOpenAI(temperature=0)` | GPT-3.5-turbo | è´Ÿè´£ç”Ÿæˆå›ç­”ã€‚temperature=0 â†’ ç¡®å®šæ€§è¾“å‡ºï¼Œä¸è¦åˆ›é€ æ€§ |
| `retriever` | ChromaDB retriever | è´Ÿè´£ä»å‘é‡åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚`k=4` è¡¨ç¤ºè¿”å›æœ€ç›¸ä¼¼çš„ 4 ä¸ªå— |
| `memory` | ConversationBufferMemory | ç»´æŠ¤å¯¹è¯å†å²ï¼Œæ”¯æŒå¤šè½®é—®ç­” |
| `return_source_documents` | `True` | åœ¨å“åº”ä¸­é™„å¸¦æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£ï¼ˆç”¨äºå±•ç¤ºå¼•ç”¨ï¼‰ |
| `output_key` | `"answer"` | æŒ‡å®š chain è¾“å‡ºä¸­å›ç­”æ–‡æœ¬çš„é”®åï¼Œä¸ memory çš„ `output_key` å¯¹åº”ï¼Œç¡®ä¿è®°å¿†æ­£ç¡®è®°å½• AI çš„å›å¤ |
| `verbose` | `True` | åœ¨ç»ˆç«¯æ‰“å° Chain çš„æ‰§è¡Œæ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰ |

**`as_retriever(search_kwargs={"k": 4})` åšäº†ä»€ä¹ˆï¼Ÿ**
æŠŠ ChromaDB å‘é‡åº“åŒ…è£…æˆä¸€ä¸ª LangChain `Retriever` å¯¹è±¡ã€‚å½“è¢«è°ƒç”¨æ—¶ï¼š
1. æ¥æ”¶æŸ¥è¯¢æ–‡æœ¬
2. ç”¨ `self.embeddings` æŠŠæŸ¥è¯¢ç¼–ç æˆå‘é‡
3. åœ¨ ChromaDB ä¸­åšä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
4. è¿”å› top-4 æœ€ç›¸ä¼¼çš„ Document å¯¹è±¡

**ConversationalRetrievalChain å†…éƒ¨çš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆé‡è¦ï¼é¢è¯•å¿…è€ƒï¼‰ï¼š**

```
ç”¨æˆ·è¾“å…¥: "Are any of those druggable?"
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Question Condensingï¼ˆé—®é¢˜é‡å†™ï¼‰       â”‚
â”‚                                              â”‚
â”‚  è¾“å…¥: å½“å‰é—®é¢˜ + chat_history                 â”‚
â”‚    "Are any of those druggable?"             â”‚
â”‚    + [ä¹‹å‰é—®äº† "What pathways regulate        â”‚
â”‚       oocyte maturation?"]                    â”‚
â”‚                                              â”‚
â”‚  â†’ LLM é‡å†™ä¸ºç‹¬ç«‹é—®é¢˜:                         â”‚
â”‚    "Are any pathways that regulate oocyte     â”‚
â”‚     maturation associated with druggable      â”‚
â”‚     targets?"                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Retrievalï¼ˆå‘é‡æ£€ç´¢ï¼‰                 â”‚
â”‚                                              â”‚
â”‚  ç”¨é‡å†™åçš„ç‹¬ç«‹é—®é¢˜å» ChromaDB æœç´¢              â”‚
â”‚  â†’ è¿”å› top-4 æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—                    â”‚
â”‚  æ¯ä¸ªå—å¸¦æœ‰ page_content å’Œ metadata           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Generationï¼ˆç­”æ¡ˆç”Ÿæˆï¼‰                â”‚
â”‚                                              â”‚
â”‚  æŠŠ 4 ä¸ªæ–‡æœ¬å— + é‡å†™åçš„é—®é¢˜ ä¸€èµ·å‘ç»™ GPT-3.5  â”‚
â”‚  Prompt å¤§è‡´æ˜¯:                                â”‚
â”‚    "Based on the following context, answer    â”‚
â”‚     the question.                             â”‚
â”‚     Context: [4ä¸ªæ–‡æœ¬å—]                       â”‚
â”‚     Question: [é‡å†™åçš„é—®é¢˜]"                   â”‚
â”‚                                              â”‚
â”‚  â†’ GPT-3.5 åŸºäºè¿™äº›ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Memory Updateï¼ˆè®°å¿†æ›´æ–°ï¼‰             â”‚
â”‚                                              â”‚
â”‚  æŠŠè¿™è½®çš„é—®ç­”å¯¹è¿½åŠ åˆ° chat_history               â”‚
â”‚  ä¸‹æ¬¡æé—®æ—¶ä¼šå¸¦ä¸Šå®Œæ•´çš„å¯¹è¯å†å²                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆ Step 1ï¼ˆé—®é¢˜é‡å†™ï¼‰æ˜¯å…³é”®ï¼Ÿ**

æ²¡æœ‰é—®é¢˜é‡å†™çš„è¯ï¼š
- ç”¨æˆ·é—®ï¼š"Are any of those druggable?"
- "those" å¯¹å‘é‡åº“æ¥è¯´æ¯«æ— æ„ä¹‰
- æ£€ç´¢ç»“æœä¼šå®Œå…¨ä¸ç›¸å…³
- å›ç­”è´¨é‡å´©æºƒ

æœ‰é—®é¢˜é‡å†™åï¼š
- LLM æŠŠ "those" è§£æä¸ºä¹‹å‰æåˆ°çš„ pathways
- ç”Ÿæˆç‹¬ç«‹é—®é¢˜ï¼š"Are any pathways regulating oocyte maturation druggable?"
- å‘é‡åº“æ£€ç´¢åˆ°æ­£ç¡®çš„æ–‡æ¡£
- å›ç­”è´¨é‡ä¿æŒé«˜æ°´å¹³

#### `ask` æ–¹æ³•

```python
def ask(self, query: str):
    response = self.qa_chain({"question": query})
    return response
```

**è¾“å…¥ï¼š**
| å‚æ•° | ç±»å‹ | ä¾‹å­ |
|------|------|------|
| `query` | `str` | `"What signaling pathways regulate oocyte maturation?"` |

**è¾“å‡ºï¼š**
| è¿”å›å€¼ | ç±»å‹ | ç»“æ„ |
|--------|------|------|
| `response` | `dict` | åŒ…å«ä¸‰ä¸ªé”®ï¼ˆè§ä¸‹è¡¨ï¼‰ |

```python
{
    "question": "What signaling pathways regulate oocyte maturation?",
    "answer": "Based on the literature, several signaling pathways regulate oocyte maturation, including the MAPK/ERK pathway, PI3K/AKT pathway, and BMP/SMAD pathway...",
    "source_documents": [
        Document(page_content="...", metadata={"source": "paper1.pdf", "page": 3}),
        Document(page_content="...", metadata={"source": "paper2.pdf", "page": 7}),
        Document(page_content="...", metadata={"source": "paper1.pdf", "page": 5}),
        Document(page_content="...", metadata={"source": "paper3.pdf", "page": 2})
    ]
}
```

#### `__main__` éƒ¨åˆ†

```python
if __name__ == '__main__':
    from src.embeddings import VectorStoreManager
    vector_store_manager = VectorStoreManager()
    vector_store = vector_store_manager.load_vector_store()
    rag = RAGPipeline(vector_store)
    while True:
        user_input = input("Please type your question:")
        if user_input.lower() == 'exit':
            break
        result = rag.ask(user_input)
        print("Answerï¼š", result['answer'])
```

è¿™æ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œæµ‹è¯•æ¥å£ã€‚ç›´æ¥è¿è¡Œ `python src/rag_pipeline.py` å°±å¯ä»¥åœ¨ç»ˆç«¯é‡Œå’Œç³»ç»Ÿå¯¹è¯ï¼Œä¸éœ€è¦å¯åŠ¨ Streamlitã€‚æ–¹ä¾¿è°ƒè¯•ã€‚æ³¨æ„è¿™é‡Œé€šè¿‡ `VectorStoreManager` åŠ è½½å‘é‡åº“å†ä¼ ç»™ `RAGPipeline`ï¼Œä¿æŒäº†å’Œ `app.py` ä¸€è‡´çš„åˆå§‹åŒ–æ–¹å¼ã€‚

### é¢è¯•é—®ç­”

---

**Q: Walk me through what happens internally when a user asks a question.**

> "When the user submits a query, the ConversationalRetrievalChain executes a three-step pipeline. First, question condensing: if there's any conversation history, the chain sends the current question along with the chat history to the LLM and asks it to rewrite the question as a standalone query. For example, 'How does it affect fertility?' becomes 'How does BMP15 affect oocyte fertility?' This is critical because the vector database has no concept of conversational context.
>
> Second, retrieval: the condensed question is embedded using OpenAI's embedding model into a 1536-dimensional vector, then ChromaDB performs a cosine similarity search and returns the top-4 most similar document chunks. Each chunk carries metadata including the source file and page number.
>
> Third, generation: the 4 retrieved chunks are injected into a prompt template as context, along with the condensed question. GPT-3.5-turbo generates an answer grounded in that context. Finally, the memory module stores this Q&A pair for future turns."

---

**Q: What's the problem with ConversationBufferMemory? How would you fix it?**

> "ConversationBufferMemory stores the entire conversation history verbatim. Every human message and AI response is appended to a growing list. After 10-15 exchanges, this can consume several thousand tokens, eating into GPT-3.5-turbo's 16K context window and leaving less room for retrieved documents and the actual answer.
>
> I'd fix this in stages. The quickest fix is ConversationBufferWindowMemory with k=5 â€” keep only the last 5 exchanges, drop everything older. A more sophisticated approach is ConversationSummaryBufferMemory â€” it keeps recent messages verbatim but summarizes older ones using the LLM, compressing 'we discussed BMP15, SMAD signaling, and oocyte maturation pathways' into a single sentence. The best production approach is a hybrid: recent 3 turns kept in full, everything older summarized, with a hard token cap."

---

**Q: Why `temperature=0`? Would you ever change it?**

> "Temperature controls the probability distribution over the next token during generation. At zero, the model always picks the most probable token â€” effectively greedy decoding. This makes output deterministic and maximally factual, which is essential for scientific Q&A. A researcher asking about cell signaling pathways needs a reproducible, evidence-based answer, not creative prose.
>
> I'd increase temperature in specific scenarios: hypothesis generation, where you want the model to suggest non-obvious connections, maybe 0.3 to 0.5. Or diverse summarization, where you want multiple distinct phrasings of the same concept. But for any customer-facing QIAGEN product serving pharma researchers, I'd default to low temperature. Reproducibility is a requirement, not a nice-to-have."

---

**Q: What does `return_source_documents=True` give you?**

> "It instructs the chain to include the actual Document objects that were retrieved from ChromaDB in the response dictionary. Each Document has two attributes: `page_content`, which is the raw text of that chunk, and `metadata`, which contains the source PDF filename and page number. This is what powers the citation feature in the UI â€” I can show users not just the answer, but exactly which passages from which papers the answer is based on. In a pharma context, this traceability is non-negotiable. A scientist needs to verify any AI-generated claim against the primary source."

---

## 5. `process_pdfs.py`

è¿™æ˜¯ä¸€ä¸ª**ä¸€æ¬¡æ€§è¿è¡Œçš„è„šæœ¬**â€”â€”æ‰§è¡Œé˜¶æ®µä¸€ï¼ˆç¦»çº¿é¢„å¤„ç†ï¼‰ï¼ŒæŠŠ PDF å˜æˆå‘é‡åº“ã€‚

### å®Œæ•´æºç ï¼ˆ49 è¡Œï¼‰

```python
from src.document_loader import DocumentProcessor
from src.embeddings import VectorStoreManager
import os

def main():
    pdf_directory = "data/papers"

    if not os.path.exists(pdf_directory):
        os.makedirs(pdf_directory)
        print(f"Created directory {pdf_directory}")
        print(f"Please place your PDF files in {pdf_directory} directory and run this script again.")
        return

    pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print(f"No PDF files found in {pdf_directory}!")
        print("Please add some PDF files to this directory and run this script again.")
        return

    print(f"Found {len(pdf_files)} PDF files: {', '.join(pdf_files)}")

    print("Processing PDF documents...")
    document_processor = DocumentProcessor()
    documents = document_processor.load_pdfs(pdf_directory)

    if not documents:
        print("Error: No document chunks were generated!")
        return

    print(f"Successfully processed {len(documents)} document chunks.")

    print("Creating vector store (this may take a while)...")
    try:
        vector_store_manager = VectorStoreManager()
        vector_store = vector_store_manager.create_vector_store(documents)
        print("Vector store created successfully!")
        print("You can now run the Streamlit app and query your documents.")
    except Exception as e:
        print(f"Error creating vector store: {str(e)}")
        print("Check your OpenAI API key and ensure it's correctly set in your .env file.")

if __name__ == "__main__":
    main()
```

### é€å—æ‹†è§£

è¿™ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ª**ç¼–æ’è„šæœ¬**â€”â€”å®ƒæœ¬èº«ä¸åŒ…å«æ–°é€»è¾‘ï¼Œè€Œæ˜¯æŒ‰æ­£ç¡®çš„é¡ºåºè°ƒç”¨å‰é¢ä¸¤ä¸ªæ¨¡å—ã€‚

**å®Œæ•´æ‰§è¡Œæµç¨‹ï¼š**

```
python process_pdfs.py
        â†“
æ£€æŸ¥ data/papers/ ç›®å½•æ˜¯å¦å­˜åœ¨ â†’ ä¸å­˜åœ¨å°±åˆ›å»ºå¹¶æç¤ºç”¨æˆ·æ”¾å…¥ PDF
        â†“
æ£€æŸ¥ç›®å½•é‡Œæœ‰æ²¡æœ‰ .pdf æ–‡ä»¶ â†’ æ²¡æœ‰å°±æç¤ºç”¨æˆ·æ·»åŠ 
        â†“
DocumentProcessor().load_pdfs("data/papers")
  â†’ è¯»å–æ‰€æœ‰ PDF â†’ æå–æ–‡æœ¬ â†’ åˆ‡åˆ†æˆ ~1000 å­—ç¬¦çš„å—
  â†’ è¿”å› List[Document]
        â†“
æ‰“å° "Successfully processed X document chunks."
        â†“
VectorStoreManager().create_vector_store(documents)
  â†’ å¯¹æ¯ä¸ªå—è°ƒç”¨ OpenAI Embedding API â†’ å­˜å…¥ ChromaDB â†’ æŒä¹…åŒ–åˆ°ç£ç›˜
        â†“
æ‰“å° "Vector store created successfully!"
        â†“
ç°åœ¨å¯ä»¥è¿è¡Œ streamlit run app.py äº†
```

**ä¸ºä»€ä¹ˆè¦æŠŠé¢„å¤„ç†ç‹¬ç«‹æˆä¸€ä¸ªè„šæœ¬ï¼Ÿ**

ä¸‰ä¸ªåŸå› ï¼š
1. **Embedding å¾ˆè´µä¹Ÿå¾ˆæ…¢ï¼š** 100 ä¸ªæ–‡æœ¬å—éœ€è¦çº¦ 100 æ¬¡ API è°ƒç”¨ã€‚å¦‚æœæ¯æ¬¡å¯åŠ¨ app éƒ½é‡æ–°ç¼–ç ï¼Œæµªè´¹æ—¶é—´å’Œé’±ã€‚
2. **æ•°æ®ä¸å¸¸å˜ï¼š** è®ºæ–‡åº“ä¸æ˜¯æ¯å¤©éƒ½æ›´æ–°çš„ã€‚é¢„å¤„ç†ä¸€æ¬¡ï¼Œå‘é‡åº“å­˜åˆ°ç£ç›˜ï¼Œapp å¯åŠ¨æ—¶ç›´æ¥åŠ è½½ã€‚
3. **å…³æ³¨ç‚¹åˆ†ç¦»ï¼š** æ•°æ®å‡†å¤‡å’Œç”¨æˆ·äº¤äº’æ˜¯ä¸¤ä¸ªä¸åŒçš„å…³æ³¨ç‚¹ã€‚åˆ†å¼€åï¼Œä½ å¯ä»¥å•ç‹¬æ›´æ–°æ•°æ®ï¼ˆåŠ æ–°è®ºæ–‡ â†’ é‡è·‘ `process_pdfs.py`ï¼‰ï¼Œè€Œä¸å½±å“ app çš„ä»£ç ã€‚

**é˜²å¾¡æ€§ç¼–ç¨‹ç»†èŠ‚ï¼š**
- æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
- æ£€æŸ¥æ˜¯å¦æœ‰ PDF æ–‡ä»¶
- æ£€æŸ¥åˆ†å—ç»“æœæ˜¯å¦ä¸ºç©º
- æ•è·å‘é‡å­˜å‚¨åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¼‚å¸¸ï¼ˆé€šå¸¸æ˜¯ API Key é—®é¢˜ï¼‰

### é¢è¯•é—®ç­”

---

**Q: Why did you separate the PDF processing from the main app?**

> "It's a classic separation of offline and online workloads. Embedding documents is an expensive, one-time operation â€” it calls the OpenAI API for every chunk and can take minutes. The web application, on the other hand, needs to start quickly and serve user queries in real time. By processing PDFs offline and persisting the vector store to disk, the app startup just loads pre-computed vectors from disk â€” instant. This also saves API costs: if I restart the app 10 times during development, I'm not re-embedding the same documents 10 times. In a production system, this separation would be even more important â€” you'd likely run the ingestion pipeline on a schedule or as a triggered job, completely decoupled from the serving layer."

---

## 6. `app.py`

è¿™æ˜¯**ç”¨æˆ·çœ‹åˆ°çš„ç•Œé¢**â€”â€”Streamlit Web åº”ç”¨ï¼ŒæŠŠæ‰€æœ‰åç«¯æ¨¡å—ä¸²æˆä¸€ä¸ªå¯äº¤äº’çš„äº§å“ã€‚

### å®Œæ•´æºç ï¼ˆçº¦ 200 è¡Œï¼‰â€” åˆ†å››ä¸ªé€»è¾‘æ®µè½è®²è§£

#### æ®µè½ 1: é…ç½®ä¸æ ·å¼ï¼ˆç¬¬ 1-49 è¡Œï¼‰

```python
import streamlit as st
from src.embeddings import VectorStoreManager
from src.rag_pipeline import RAGPipeline
from dotenv import load_dotenv
import os

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    try:
        os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
    except (KeyError, FileNotFoundError):
        st.error("Please set OPENAI_API_KEY in .env file or Streamlit Cloud Secrets.")
        st.stop()

st.set_page_config(
    page_title="Oocyte Expert",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""<style>...</style>""", unsafe_allow_html=True)
```

**åšäº†ä»€ä¹ˆï¼š**
1. **ç¯å¢ƒå˜é‡åŠ è½½ï¼š** `load_dotenv()` ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå¦‚ `OPENAI_API_KEY`ï¼‰ï¼Œæ”¯æŒæœ¬åœ°å¼€å‘ã€‚
2. **åŒé‡ API Key è·å–ï¼š** å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡ï¼ˆ`.env`ï¼‰è·å– API Keyï¼›å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•ä» Streamlit Cloud Secrets è·å–ã€‚è¿™æ ·åŒæ—¶æ”¯æŒæœ¬åœ°å¼€å‘å’Œ Streamlit Cloud éƒ¨ç½²ä¸¤ç§åœºæ™¯ã€‚å¦‚æœä¸¤è€…éƒ½æ²¡æœ‰ï¼Œæ˜¾ç¤ºé”™è¯¯å¹¶åœæ­¢ã€‚
3. **é¡µé¢é…ç½®ï¼š** `st.set_page_config()` è®¾ç½®æµè§ˆå™¨æ ‡ç­¾æ ‡é¢˜ã€å›¾æ ‡ã€é¡µé¢å¸ƒå±€ã€‚`layout="wide"` è®©é¡µé¢ä½¿ç”¨å…¨å®½è€Œä¸æ˜¯é»˜è®¤çš„å±…ä¸­çª„åˆ—ã€‚
4. **è‡ªå®šä¹‰ CSSï¼š** é€šè¿‡ `st.markdown` æ³¨å…¥ CSS æ¥ç¾åŒ–èŠå¤©ç•Œé¢ã€‚å®šä¹‰äº† `.chat-message`ã€`.user-message`ã€`.assistant-message` å’Œ `.citation` å››ä¸ªæ ·å¼ç±»ã€‚

#### æ®µè½ 2: Session State åˆå§‹åŒ–ï¼ˆç¬¬ 52-57 è¡Œï¼‰

```python
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = None
if 'is_initialized' not in st.session_state:
    st.session_state.is_initialized = False
```

**ä¸ºä»€ä¹ˆéœ€è¦ session_stateï¼Ÿ**

Streamlit çš„æ‰§è¡Œæ¨¡å‹å¾ˆç‰¹æ®Šï¼š**æ¯æ¬¡ç”¨æˆ·äº¤äº’ï¼ˆç‚¹å‡»æŒ‰é’®ã€è¾“å…¥æ–‡å­—ï¼‰ï¼Œæ•´ä¸ª `app.py` è„šæœ¬ä¼šä»å¤´åˆ°å°¾é‡æ–°æ‰§è¡Œä¸€éã€‚** æ™®é€šçš„ Python å˜é‡æ¯æ¬¡é‡æ–°æ‰§è¡Œéƒ½ä¼šè¢«é‡ç½®ã€‚`st.session_state` æ˜¯ Streamlit æä¾›çš„æŒä¹…åŒ–å­˜å‚¨â€”â€”è·¨æ¬¡è¿è¡Œä¿ç•™æ•°æ®ã€‚

| çŠ¶æ€å˜é‡ | ç±»å‹ | ç”¨é€” |
|----------|------|------|
| `chat_history` | `List[dict]` | å­˜å‚¨æ‰€æœ‰èŠå¤©æ¶ˆæ¯ï¼ˆç”¨æˆ·+AIï¼‰ï¼Œç”¨äºæ¸²æŸ“èŠå¤©ç•Œé¢ |
| `rag_pipeline` | `RAGPipeline` or `None` | RAG æµæ°´çº¿å®ä¾‹ï¼ˆå†…éƒ¨åŒ…å«å‘é‡åº“ï¼‰ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°åˆ›å»º |
| `is_initialized` | `bool` | æ ‡è®°ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åŠ è½½ |

**ä¸¾ä¾‹è¯´æ˜ Streamlit çš„é‡æ–°æ‰§è¡Œæœºåˆ¶ï¼š**

```
ç”¨æˆ·ç¬¬1æ¬¡æ‰“å¼€é¡µé¢:
  app.py æ‰§è¡Œ â†’ session_state ä¸ºç©º â†’ åˆå§‹åŒ–æ‰€æœ‰å˜é‡ â†’ åŠ è½½å‘é‡åº“ â†’ æ¸²æŸ“ç©ºèŠå¤©ç•Œé¢

ç”¨æˆ·è¾“å…¥ "What is BMP15?" å¹¶æŒ‰å›è½¦:
  app.py ä»å¤´æ‰§è¡Œ â†’ session_state å·²æœ‰æ•°æ® â†’ è·³è¿‡åˆå§‹åŒ– â†’ æ¸²æŸ“ä¹‹å‰çš„èŠå¤© â†’ å¤„ç†æ–°é—®é¢˜

ç”¨æˆ·å†è¾“å…¥ "How does it work?":
  app.py ä»å¤´æ‰§è¡Œ â†’ session_state æœ‰ 2 æ¡æ¶ˆæ¯ â†’ æ¸²æŸ“ä¹‹å‰çš„èŠå¤© â†’ å¤„ç†æ–°é—®é¢˜
```

å¦‚æœä¸ç”¨ `session_state`ï¼Œæ¯æ¬¡ç”¨æˆ·è¾“å…¥åï¼Œå¯¹è¯å†å²å’Œ RAG æµæ°´çº¿éƒ½ä¼šè¢«æ¸…ç©ºï¼Œå¤šè½®å¯¹è¯å°±ä¸å¯èƒ½å®ç°ã€‚

#### æ®µè½ 3: ä¾§è¾¹æ å’Œç³»ç»Ÿåˆå§‹åŒ–ï¼ˆç¬¬ 60-108 è¡Œï¼‰

```python
# ä¾§è¾¹æ 
with st.sidebar:
    st.title("ğŸ§¬ Oocyte Expert")
    st.markdown("""...""")

    if st.session_state.is_initialized:
        st.success("Knowledge Base: Active âœ…")
    else:
        st.warning("Knowledge Base: Loading...")

    if st.button("Reset System"):
        st.session_state.chat_history = []
        st.rerun()

# ç³»ç»Ÿåˆå§‹åŒ–ï¼ˆåªåœ¨é¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
if not st.session_state.is_initialized:
    with st.spinner("Initializing knowledge base..."):
        try:
            vector_store_manager = VectorStoreManager()
            try:
                vector_store = vector_store_manager.load_vector_store("data/chroma_db")
            except ValueError:
                st.info("Building vector store for the first time, this may take a moment...")
                from src.document_loader import DocumentProcessor
                processor = DocumentProcessor()
                documents = processor.load_pdfs("data/papers")
                if not documents:
                    st.error("No PDF documents found in data/papers/")
                    st.stop()
                vector_store = vector_store_manager.create_vector_store(documents)

            st.session_state.rag_pipeline = RAGPipeline(vector_store)
            st.session_state.is_initialized = True
        except Exception as e:
            st.error(f"Error initializing system: {str(e)}")
            st.stop()
```

**ä¾§è¾¹æ åšäº†ä»€ä¹ˆï¼š**
- å±•ç¤ºé¡¹ç›®æ ‡é¢˜å’Œç®€ä»‹
- ç”¨ `st.success` / `st.warning` æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€ï¼ˆç»¿è‰² = å°±ç»ªï¼Œé»„è‰² = åŠ è½½ä¸­ï¼‰
- "Reset System" æŒ‰é’®æ¸…ç©ºå¯¹è¯å†å²å¹¶åˆ·æ–°é¡µé¢

**åˆå§‹åŒ–æµç¨‹ï¼š**

```
is_initialized == False? (é¦–æ¬¡è®¿é—®)
        â†“ Yes
æ˜¾ç¤º spinner "Initializing knowledge base..."
        â†“
VectorStoreManager() â†’ åˆå§‹åŒ– Embedding æ¨¡å‹
        â†“
load_vector_store("data/chroma_db") â†’ å°è¯•ä»ç£ç›˜åŠ è½½å‘é‡åº“
        â†“ å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ â†’ è‡ªåŠ¨æ„å»ºå‘é‡åº“ï¼š
        â†“   1. æ˜¾ç¤º st.info æç¤ºç”¨æˆ·æ­£åœ¨æ„å»º
        â†“   2. DocumentProcessor().load_pdfs("data/papers") åŠ è½½å¹¶åˆ†å—
        â†“   3. create_vector_store(documents) ç¼–ç å¹¶æŒä¹…åŒ–
        â†“
RAGPipeline(vector_store) â†’ å¤ç”¨å·²åŠ è½½çš„å‘é‡åº“ï¼Œåˆ›å»º RAG æµæ°´çº¿
        â†“
is_initialized = True â†’ ä¸‹æ¬¡è„šæœ¬é‡æ–°æ‰§è¡Œæ—¶è·³è¿‡è¿™ä¸ªå—
```

**`st.stop()` çš„ä½œç”¨ï¼š** ç«‹å³åœæ­¢è„šæœ¬æ‰§è¡Œï¼Œé¡µé¢åªæ˜¾ç¤ºåˆ°ç›®å‰ä¸ºæ­¢æ¸²æŸ“çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ç§ä¼˜é›…çš„é”™è¯¯å¤„ç†â€”â€”å¦‚æœåœ¨è‡ªåŠ¨æ„å»ºè¿‡ç¨‹ä¸­æ‰¾ä¸åˆ° PDF æ–‡ä»¶ï¼Œä¸è¦ç»§ç»­æ¸²æŸ“èŠå¤©ç•Œé¢ã€‚

#### æ®µè½ 3.5: æ¨èé—®é¢˜ï¼ˆSuggested Questionsï¼‰

```python
SUGGESTED_QUESTIONS = {
    "OmniPath": [
        "What is OmniPath and what types of biological data does it integrate?",
        "How does OmniPath compare to other pathway databases like KEGG or Reactome?",
        "What are the main data sources combined in OmniPath?",
    ],
    "CellChat & CellPhoneDB": [
        "What is CellChat and how does it infer cell-cell communication?",
        "How does CellPhoneDB predict ligand-receptor interactions from scRNA-seq data?",
        "What are the differences between CellChat and CellPhoneDB?",
    ],
    "Oocyte Biology": [
        "What metabolites are secreted by cumulus cells during oocyte maturation?",
        "How do cumulus cells influence oocyte developmental competence?",
        "What signaling pathways regulate oocyte maturation?",
    ],
}

if not st.session_state.chat_history:
    st.markdown("### Try asking:")
    for category, questions in SUGGESTED_QUESTIONS.items():
        st.markdown(f"**{category}**")
        for q in questions:
            if st.button(q, key=q):
                st.session_state.pending_question = q
                st.rerun()
```

**åšäº†ä»€ä¹ˆï¼š**
- å®šä¹‰äº†ä¸€ç»„æŒ‰ç±»åˆ«åˆ†ç»„çš„æ¨èé—®é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå¼€å§‹å¯¹è¯
- åªåœ¨èŠå¤©å†å²ä¸ºç©ºæ—¶æ˜¾ç¤ºï¼ˆä¸€æ—¦ç”¨æˆ·å¼€å§‹å¯¹è¯ï¼Œæ¨èé—®é¢˜æ¶ˆå¤±ï¼‰
- ç”¨æˆ·ç‚¹å‡»æŸä¸ªæ¨èé—®é¢˜åï¼ŒæŠŠå®ƒå­˜å…¥ `st.session_state.pending_question`ï¼Œç„¶åè°ƒç”¨ `st.rerun()` é‡æ–°æ‰§è¡Œè„šæœ¬
- åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œä¸­ï¼ŒèŠå¤©è¾“å…¥å¤„ç†é€»è¾‘ä¼šè¯»å– `pending_question` å¹¶ä½œä¸ºç”¨æˆ·çš„æé—®å¤„ç†

#### æ®µè½ 4: èŠå¤©ç•Œé¢

```python
# æ¸²æŸ“å†å²æ¶ˆæ¯
for idx, message in enumerate(st.session_state.chat_history):
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "citations" in message and message["citations"]:
            with st.expander("View Citations"):
                for citation in message["citations"]:
                    st.markdown(f"*{citation}*")

# å¤„ç†æ¨èé—®é¢˜ç‚¹å‡»æˆ–ç”¨æˆ·è¾“å…¥
pending = st.session_state.pop("pending_question", None)
typed = st.chat_input("Ask your question about oocyte research...")
prompt = pending or typed

if prompt:
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        if not st.session_state.rag_pipeline:
            st.error("System not initialized. Please wait...")
        else:
            with st.spinner("Researching..."):
                try:
                    response = st.session_state.rag_pipeline.ask(prompt)
                    answer = response["answer"]
                    sources = response.get("source_documents", [])

                    citations = []
                    for doc in sources:
                        src = doc.metadata.get("source", "")
                        page = doc.metadata.get("page", "")
                        citations.append(f"{src}, Page {int(page) + 1}")

                    st.session_state.chat_history.append({
                        "role": "assistant",
                        "content": answer,
                        "citations": citations
                    })
                    st.write(answer)
                except Exception as e:
                    st.error(f"Error generating response: {str(e)}")

# åº•éƒ¨æŒ‰é’®
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if st.button("Clear Conversation"):
        st.session_state.chat_history = []
        st.rerun()
with col2:
    if st.button("Export Chat"):
        st.info("Export feature coming soon!")
```

**èŠå¤©æ¸²æŸ“æµç¨‹ï¼š**

```
ç¬¬1æ­¥: éå† chat_historyï¼Œæ¸²æŸ“ä¹‹å‰æ‰€æœ‰çš„å¯¹è¯æ¶ˆæ¯
       æ¯æ¡æ¶ˆæ¯ç”¨ st.chat_message() æ˜¾ç¤ºå¯¹åº”çš„å¤´åƒï¼ˆuser/assistantï¼‰
       å¦‚æœæ¶ˆæ¯æœ‰ citationsï¼Œç”¨ st.expander æŠ˜å å±•ç¤º
       â†“
ç¬¬2æ­¥: æ£€æŸ¥æ˜¯å¦æœ‰ pending_questionï¼ˆæ¥è‡ªæ¨èé—®é¢˜ç‚¹å‡»ï¼‰
       ä»¥åŠ st.chat_input() æ˜¾ç¤ºè¾“å…¥æ¡†ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥
       prompt = pending or typed
       â†“ ç”¨æˆ·è¾“å…¥ "What is BMP15?" å¹¶æŒ‰å›è½¦ï¼ˆæˆ–ç‚¹å‡»æ¨èé—®é¢˜ï¼‰
       â†“
ç¬¬3æ­¥: æŠŠç”¨æˆ·æ¶ˆæ¯è¿½åŠ åˆ° chat_history
       â†“
ç¬¬4æ­¥: è°ƒç”¨ rag_pipeline.ask(prompt)
       â†’ Question Condensing â†’ Retrieval â†’ Generation
       â†“
ç¬¬5æ­¥: ä» response ä¸­æå– answer å’Œ source_documents
       éå† source_documentsï¼Œæå–æ¯ä¸ªæ–‡æ¡£çš„ source æ–‡ä»¶åå’Œ page é¡µç 
       ç”Ÿæˆå¼•ç”¨åˆ—è¡¨ï¼ˆå¦‚ "paper1.pdf, Page 5"ï¼‰
       â†“
ç¬¬6æ­¥: æŠŠ AI å›å¤è¿½åŠ åˆ° chat_historyï¼ˆå¸¦çœŸå® citationsï¼‰
       â†“
ç¬¬7æ­¥: st.write(answer) æ˜¾ç¤ºå›ç­”
       â†“
ç¬¬8æ­¥: è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œé¡µé¢æ›´æ–°ï¼Œæ˜¾ç¤ºå®Œæ•´å¯¹è¯
```

**è¾“å…¥å¤„ç†ï¼š`pending` vs `typed`**

```python
pending = st.session_state.pop("pending_question", None)
typed = st.chat_input("Ask your question about oocyte research...")
prompt = pending or typed
```

è¿™é‡Œç”¨ä¸¤æ­¥è·å–ç”¨æˆ·è¾“å…¥ï¼š
- `pending`ï¼šæ¥è‡ªæ¨èé—®é¢˜çš„ç‚¹å‡»ï¼ˆä¸Šä¸€è½® rerun æ—¶å­˜å…¥ `session_state.pending_question`ï¼‰ã€‚`pop()` å–å‡ºåç«‹å³åˆ é™¤ï¼Œé¿å…é‡å¤å¤„ç†ã€‚
- `typed`ï¼šç”¨æˆ·åœ¨è¾“å…¥æ¡†ä¸­æ‰‹åŠ¨è¾“å…¥çš„æ–‡æœ¬ã€‚
- `prompt = pending or typed`ï¼šä¼˜å…ˆå¤„ç†æ¨èé—®é¢˜ç‚¹å‡»ï¼Œå¦åˆ™å¤„ç†æ‰‹åŠ¨è¾“å…¥ã€‚ä¸¤è€…éƒ½ä¸º `None` æ—¶ä¸æ‰§è¡Œã€‚

**åº•éƒ¨æŒ‰é’®ï¼š**
- **Clear Conversationï¼š** æ¸…ç©º `chat_history` å¹¶è°ƒç”¨ `st.rerun()` å¼ºåˆ¶é‡æ–°æ‰§è¡Œè„šæœ¬ï¼Œè¿™æ ·ç•Œé¢ç«‹åˆ»æ›´æ–°ã€‚
- **Export Chatï¼š** å ä½åŠŸèƒ½ï¼Œç›®å‰åªæ˜¾ç¤º "coming soon" æç¤ºã€‚

### é¢è¯•é—®ç­”

---

**Q: Explain how Streamlit's execution model works and how it affects your architecture.**

> "Streamlit has a unique execution model: every time a user interacts with the app â€” clicks a button, types in the chat input, toggles a checkbox â€” the entire Python script re-executes from top to bottom. This means any regular Python variable is reset on every interaction. That's why `st.session_state` is essential â€” it's a persistent key-value store that survives across re-executions within the same browser session.
>
> This model has architectural implications. Heavy initialization â€” like loading the vector store and creating the RAG pipeline â€” must be gated behind an `is_initialized` flag in session_state, otherwise it would re-run on every keystroke. The chat history must also live in session_state so previous messages aren't lost. And the RAG pipeline itself â€” including its conversation memory â€” must be stored in session_state so multi-turn context is preserved.
>
> The benefit of this model is simplicity: the script reads linearly, top to bottom, like a page layout. The trade-off is that you have to be explicit about what state persists and what doesn't."

---

**Q: How does the citation feature work?**

> "The citation feature extracts real source information from the retrieved documents. When `RAGPipeline.ask()` returns a response, it includes a `source_documents` list where each document carries `metadata` with the source PDF filename and page number. The app iterates over these documents, extracts the `source` and `page` fields from each document's metadata, and formats them as readable citations like 'paper1.pdf, Page 5'. The page number is incremented by 1 since PyPDFLoader uses zero-based page indexing. These citations are stored alongside the assistant's response in `chat_history` and displayed in an expandable `st.expander('View Citations')` panel beneath each answer. This gives researchers full traceability â€” they can verify any AI-generated claim against the specific passage in the original paper."

---

**Q: What happens if the vector store doesn't exist when the app starts?**

> "The app handles this gracefully through an auto-build fallback. In the initialization block, it first tries to call `load_vector_store('data/chroma_db')`. If that directory doesn't exist, the method raises a `ValueError`. Instead of stopping, the app catches this exception and automatically builds the vector store on the fly: it imports `DocumentProcessor`, loads and splits all PDFs from `data/papers/`, and calls `create_vector_store()` to embed and persist them. The user sees an informational message â€” 'Building vector store for the first time, this may take a moment...' â€” and the app continues normally once the build completes. This eliminates the need to run `process_pdfs.py` separately before launching the app. The only hard failure is if no PDF documents are found in `data/papers/`, in which case it displays an error and stops."

---

## 7. `requirements.txt`

```
streamlit>=1.40.0
langchain>=0.3.0
langchain-community>=0.3.0
langchain-openai>=0.2.0
openai>=1.60.0
chromadb>=0.5.0
python-dotenv>=1.0.0
pypdf>=4.0.0
tiktoken>=0.7.0
```

æ¯ä¸ªä¾èµ–çš„ä½œç”¨ï¼š

| åŒ… | ç‰ˆæœ¬ | ç”¨åœ¨å“ªé‡Œ | åšä»€ä¹ˆ |
|----|------|---------|--------|
| `streamlit` | >=1.40.0 | `app.py` | Web UI æ¡†æ¶ï¼Œæä¾› chat ç»„ä»¶ã€session stateã€éƒ¨ç½² |
| `langchain` | >=0.3.0 | å…¨éƒ¨ `src/` | LLM åº”ç”¨ç¼–æ’æ¡†æ¶ï¼Œæä¾›æ ¸å¿ƒæŠ½è±¡å’Œç¼–æ’é€»è¾‘ |
| `langchain-community` | >=0.3.0 | `src/` | LangChain ç¤¾åŒºé›†æˆåŒ…ï¼Œæä¾› PyPDFLoaderã€Chroma ç­‰ |
| `langchain-openai` | >=0.2.0 | `src/` | LangChain çš„ OpenAI ä¸“ç”¨é›†æˆï¼Œæä¾› ChatOpenAI å’Œ OpenAIEmbeddings |
| `openai` | >=1.60.0 | (é—´æ¥) | OpenAI Python SDKï¼Œlangchain-openai åº•å±‚ä¾èµ– |
| `chromadb` | >=0.5.0 | `src/embeddings.py` | å‘é‡æ•°æ®åº“ï¼Œå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£åµŒå…¥ï¼ˆ0.4+ è‡ªåŠ¨æŒä¹…åŒ–ï¼‰ |
| `python-dotenv` | >=1.0.0 | `src/embeddings.py`, `app.py` | ä» `.env` æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆAPI Keyï¼‰ |
| `pypdf` | >=4.0.0 | (é—´æ¥) | PyPDFLoader åº•å±‚ä¾èµ–ï¼Œè´Ÿè´£ PDF æ–‡ä»¶è§£æï¼ˆæ›¿ä»£å·²å¼ƒç”¨çš„ pypdf2ï¼‰ |
| `tiktoken` | >=0.7.0 | (é—´æ¥) | OpenAI çš„ tokenizerï¼ŒLangChain ç”¨å®ƒæ¥è®¡ç®— token æ•° |

### é¢è¯•é—®ç­”

---

**Q: Why do you use `>=` version ranges instead of exact pins in requirements.txt?**

> "I switched from exact version pinning (`==`) to minimum version constraints (`>=`) for pragmatic reasons. With exact pins, every security patch or bug fix requires manually updating the version. Using `>=` with a tested minimum version â€” like `langchain>=0.3.0` â€” allows automatic adoption of compatible updates while guaranteeing the minimum feature set I depend on. LangChain's ecosystem has stabilized significantly since 0.3.x, with clearer separation between `langchain`, `langchain-community`, and `langchain-openai`, so the risk of breaking changes within a major version is lower. In a production environment, I'd complement this with a lock file â€” like `pip-compile` or Poetry's `poetry.lock` â€” to pin exact resolved versions for fully reproducible builds, while keeping `requirements.txt` as the looser specification."

---

## 8. ç«¯åˆ°ç«¯æ•°æ®æµæ€»ç»“

æŠŠæ‰€æœ‰æ–‡ä»¶ä¸²èµ·æ¥ï¼Œçœ‹ä¸€ä¸ªå®Œæ•´çš„ç”¨æˆ·æ—…ç¨‹ï¼š

### æ—…ç¨‹ 1: ç®¡ç†å‘˜å‡†å¤‡çŸ¥è¯†åº“ï¼ˆè·‘ä¸€æ¬¡ï¼Œæˆ–ç”± app è‡ªåŠ¨æ„å»ºï¼‰

```
ç®¡ç†å‘˜æŠŠ 3 ç¯‡è®ºæ–‡æ”¾è¿› data/papers/
         â†“
æ–¹å¼ A: è¿è¡Œ python process_pdfs.pyï¼ˆç¦»çº¿é¢„å¤„ç†ï¼‰
æ–¹å¼ B: ç›´æ¥å¯åŠ¨ app.pyï¼Œå¦‚æœå‘é‡åº“ä¸å­˜åœ¨ä¼šè‡ªåŠ¨æ„å»º
         â†“
process_pdfs.py æˆ– app.py è‡ªåŠ¨æ„å»ºæµç¨‹:
  â”‚
  â”œâ”€ DocumentProcessor.__init__()
  â”‚    â””â”€ åˆ›å»º TextSplitter(chunk_size=1000, overlap=200)
  â”‚
  â”œâ”€ DocumentProcessor.load_pdfs("data/papers")
  â”‚    â”œâ”€ DirectoryLoader æ‰¾åˆ° 3 ä¸ª PDF
  â”‚    â”œâ”€ PyPDFLoader æå–æ¯é¡µæ–‡æœ¬ â†’ çº¦ 30 ä¸ª Document
  â”‚    â””â”€ TextSplitter åˆ‡åˆ† â†’ çº¦ 100 ä¸ª Document å—
  â”‚
  â”œâ”€ VectorStoreManager.__init__()
  â”‚    â”œâ”€ load_dotenv() è¯»å– .env æ–‡ä»¶
  â”‚    â””â”€ OpenAIEmbeddings() åˆå§‹åŒ– embedding æ¨¡å‹
  â”‚
  â””â”€ VectorStoreManager.create_vector_store(100ä¸ªå—)
       â”œâ”€ Chroma.from_documents():
       â”‚    â”œâ”€ å¯¹ 100 ä¸ªå—é€ä¸ªè°ƒç”¨ OpenAI Embedding API
       â”‚    â”‚    æ¯ä¸ªå— â†’ 1536 ç»´å‘é‡
       â”‚    â””â”€ å­˜å…¥ ChromaDB å¹¶è‡ªåŠ¨æŒä¹…åŒ–åˆ° data/chroma_db/ï¼ˆSQLite + ç´¢å¼•æ–‡ä»¶ï¼‰
       â””â”€ æ‰“å° "Vector store created successfully!"
```

### æ—…ç¨‹ 2: ç”¨æˆ·æé—®ï¼ˆæ¯æ¬¡äº¤äº’ï¼‰

```
ç”¨æˆ·æ‰“å¼€æµè§ˆå™¨è®¿é—® Streamlit åº”ç”¨
         â†“
app.py é¦–æ¬¡æ‰§è¡Œ:
  â”œâ”€ load_dotenv() åŠ è½½ .env ç¯å¢ƒå˜é‡
  â”œâ”€ æ£€æŸ¥ OPENAI_API_KEYï¼ˆå…ˆ .envï¼Œå† Streamlit Cloud Secretsï¼‰
  â”œâ”€ session_state åˆå§‹åŒ–ï¼ˆç©ºåˆ—è¡¨ã€Noneã€Falseï¼‰
  â”œâ”€ VectorStoreManager().load_vector_store("data/chroma_db")
  â”‚    â””â”€ ä»ç£ç›˜åŠ è½½å‘é‡åº“ï¼ˆä¸è°ƒ APIï¼Œå¾ˆå¿«ï¼‰
  â”‚    â””â”€ å¦‚æœä¸å­˜åœ¨ â†’ è‡ªåŠ¨æ„å»ºï¼šåŠ è½½ data/papers/ ä¸­çš„ PDF â†’ ç¼–ç  â†’ æŒä¹…åŒ–
  â”œâ”€ RAGPipeline(vector_store) â€” å¤ç”¨å·²åŠ è½½çš„å‘é‡åº“å®ä¾‹
  â”‚    â”œâ”€ self.vector_store = vector_store â€” ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å®ä¾‹
  â”‚    â”œâ”€ ConversationBufferMemory(output_key="answer") â€” ç©ºçš„å¯¹è¯è®°å¿†
  â”‚    â””â”€ ConversationalRetrievalChain(output_key="answer") â€” ç»„è£…å®Œæ•´é“¾
  â””â”€ is_initialized = True
         â†“
æ˜¾ç¤ºæ¨èé—®é¢˜ï¼ˆSUGGESTED_QUESTIONSï¼‰ï¼Œæˆ–ç”¨æˆ·ç›´æ¥è¾“å…¥
ç”¨æˆ·è¾“å…¥: "What pathways regulate oocyte maturation?"
         â†“
app.py é‡æ–°æ‰§è¡Œ:
  â”œâ”€ is_initialized == True â†’ è·³è¿‡åˆå§‹åŒ–
  â”œâ”€ æ¸²æŸ“ç©ºçš„èŠå¤©ç•Œé¢
  â”œâ”€ pending_question æˆ– st.chat_input æ•è·ç”¨æˆ·è¾“å…¥
  â”œâ”€ è¿½åŠ ç”¨æˆ·æ¶ˆæ¯åˆ° chat_history
  â””â”€ rag_pipeline.ask("What pathways regulate oocyte maturation?")
       â”‚
       â”œâ”€ ConversationalRetrievalChain æ‰§è¡Œ:
       â”‚    â”‚
       â”‚    â”œâ”€ Step 1: Question Condensing
       â”‚    â”‚    chat_history ä¸ºç©º â†’ ç›´æ¥ç”¨åŸé—®é¢˜
       â”‚    â”‚
       â”‚    â”œâ”€ Step 2: Retrieval
       â”‚    â”‚    â”œâ”€ OpenAI Embedding API: é—®é¢˜ â†’ 1536 ç»´å‘é‡
       â”‚    â”‚    â”œâ”€ ChromaDB: ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ top-4
       â”‚    â”‚    â””â”€ è¿”å› 4 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
       â”‚    â”‚
       â”‚    â”œâ”€ Step 3: Generation
       â”‚    â”‚    â”œâ”€ Prompt = "Based on context: [4ä¸ªå—], answer: [é—®é¢˜]"
       â”‚    â”‚    â””â”€ GPT-3.5-turbo ç”Ÿæˆå›ç­”
       â”‚    â”‚
       â”‚    â””â”€ Step 4: Memory Update
       â”‚         â””â”€ å­˜å‚¨ Q&A å¯¹åˆ° chat_historyï¼ˆé€šè¿‡ output_key="answer"ï¼‰
       â”‚
       â””â”€ è¿”å› {"answer": "Several pathways...", "source_documents": [...]}
         â†“
æå– answer å’Œ source_documents â†’ æ„å»º citations åˆ—è¡¨
st.write(answer) â†’ åœ¨ç•Œé¢æ˜¾ç¤ºå›ç­”
è¿½åŠ  AI æ¶ˆæ¯åˆ° chat_historyï¼ˆå¸¦çœŸå®å¼•ç”¨ä¿¡æ¯ï¼‰
         â†“
ç”¨æˆ·æ¥ç€é—®: "Are any of those druggable?"
         â†“
app.py é‡æ–°æ‰§è¡Œ:
  â”œâ”€ æ¸²æŸ“ä¹‹å‰çš„ 2 æ¡æ¶ˆæ¯ï¼ˆå«å¯å±•å¼€çš„å¼•ç”¨ï¼‰
  â””â”€ rag_pipeline.ask("Are any of those druggable?")
       â”‚
       â””â”€ Step 1: Question Condensing
            â”œâ”€ chat_history = [ä¹‹å‰çš„ Q&A]
            â”œâ”€ LLM é‡å†™: "Are any pathways that regulate oocyte maturation druggable?"
            â””â”€ ç”¨é‡å†™åçš„é—®é¢˜å»æ£€ç´¢ â†’ ç”Ÿæˆ â†’ è¿”å›å›ç­”
```

### é¢è¯•ç»ˆæé—®ç­”

---

**Q: If I gave you 10 minutes to demo this system to a pharma customer, how would you structure it?**

> "I'd structure the 10 minutes into three acts. Act one â€” the problem, 2 minutes: 'Your team published three papers on oocyte biology. You want to ask questions across all of them simultaneously, with citations. Today that requires reading each paper end to end.' Act two â€” the solution, 6 minutes: live demo. I'd ask a domain-specific question like 'What signaling pathways regulate oocyte maturation?' and show the system returning an evidence-based answer with source citations. Then I'd ask a follow-up â€” 'Are any of those pathways druggable?' â€” to demonstrate multi-turn context. I'd point out that the system remembered what 'those' refers to. Act three â€” the bridge, 2 minutes: 'This is what I built with three papers and vector search. Imagine this at the scale of QIAGEN's Biomedical Knowledge Base â€” millions of curated findings, explicit pathway relationships, and graph-based reasoning. That's the step function improvement we can offer your team.'"

---

**Q: What's the single biggest weakness of this system?**

> "The system retrieves text chunks based on semantic similarity but has no understanding of biological entity relationships. If I ask 'What is upstream of SMAD signaling in oocyte maturation?', the system can only find chunks that happen to mention both 'upstream' and 'SMAD' â€” it doesn't actually traverse a signaling pathway. A knowledge graph would let me do exactly that: start at the SMAD node, follow 'activated_by' edges, and return the upstream regulators. That's the fundamental difference between what BioRAG does with vector search and what QIAGEN does with a curated biomedical knowledge graph. My project is a proof of concept for one half of the equation. QIAGEN has the other half â€” and the combination of both is where the real value lies."

---

> **å¤ä¹ å»ºè®®ï¼š** é¢è¯•å‰æŠŠè¿™ä»½æ–‡æ¡£è¯»ä¸¤éã€‚ç¬¬ä¸€éè¯»è§£é‡Šï¼Œç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ã€‚ç¬¬äºŒéåªè¯»é¢è¯•é—®ç­”éƒ¨åˆ†ï¼Œç»ƒä¹ ç”¨è‹±æ–‡å›ç­”ã€‚é‡ç‚¹è®°ä½ ConversationalRetrievalChain çš„ä¸‰æ­¥å†…éƒ¨æµç¨‹ï¼ˆquestion condensing â†’ retrieval â†’ generationï¼‰ï¼Œè¿™æ˜¯æœ€é«˜é¢‘çš„æŠ€æœ¯é¢è¯•é¢˜ã€‚
